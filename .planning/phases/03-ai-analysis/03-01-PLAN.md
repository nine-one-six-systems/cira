---
phase: 03-ai-analysis
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - backend/tests/test_analysis_integration.py
  - backend/tests/fixtures/analysis_fixtures.py
autonomous: true

must_haves:
  truths:
    - "Analysis pipeline processes crawled content end-to-end"
    - "Claude API called with properly formatted prompts for each section"
    - "All 7 analysis sections generated (executive_summary, company_overview, business_model, team_leadership, market_position, key_insights, red_flags)"
    - "Token usage tracked per section with correct input/output counts"
    - "Cost estimation calculated from token usage"
    - "Progress callbacks invoked during analysis"
  artifacts:
    - path: "backend/tests/test_analysis_integration.py"
      provides: "Integration tests for full analysis pipeline"
      min_lines: 250
    - path: "backend/tests/fixtures/analysis_fixtures.py"
      provides: "Shared fixtures for analysis testing"
      min_lines: 100
  key_links:
    - from: "AnalysisSynthesizer"
      to: "AnthropicService"
      via: "anthropic_service.call()"
      pattern: "anthropic_service\\.call"
    - from: "AnalysisSynthesizer"
      to: "TokenTracker"
      via: "token_tracker.record_usage()"
      pattern: "token_tracker\\.record_usage"
    - from: "analyze_content task"
      to: "AnalysisSynthesizer"
      via: "run_full_analysis()"
      pattern: "run_full_analysis"
---

<objective>
Create integration tests for the AI analysis pipeline that verify all components work together correctly.

Purpose: Validate that existing Claude API integration, section generation, and token tracking implementation meets Phase 3 requirements (ANA-01 through ANA-10) when components are integrated, catching wiring issues that unit tests miss.

Output: Integration test file with mock Claude responses exercising full analysis flow.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/03-ai-analysis/03-RESEARCH.md
@backend/app/services/anthropic_service.py
@backend/app/services/token_tracker.py
@backend/app/analysis/prompts.py
@backend/app/analysis/synthesis.py
@backend/app/workers/tasks.py
@backend/tests/conftest.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create analysis integration test fixtures</name>
  <files>backend/tests/fixtures/analysis_fixtures.py</files>
  <action>
Create shared fixtures for analysis integration testing at `backend/tests/fixtures/analysis_fixtures.py`:

1. If `backend/tests/fixtures/__init__.py` doesn't exist, create it (empty file)

2. Create `backend/tests/fixtures/analysis_fixtures.py` with:

   - `MOCK_CRAWLED_CONTENT`: Dict simulating content prepared for analysis containing:
     - `about`: About page text (200+ words) with company mission, founding story
     - `team`: Team page text (200+ words) with leadership names and roles
     - `products`: Products page text (150+ words) with product descriptions
     - `news`: News/press page text (100+ words) with recent announcements

   - `MOCK_ENTITIES`: List of mock extracted entities with:
     - 3-4 PERSON entities with roles (CEO, CTO, VP Engineering)
     - 2-3 ORG entities (investors, partners)
     - 2 GPE entities (headquarters, office locations)
     - 3-4 PRODUCT entities

   - `MOCK_CLAUDE_RESPONSE`: Factory function that generates realistic section content:
     ```python
     def mock_claude_response(section_id: str) -> dict:
         # Returns dict with content for each section type
         # e.g., executive_summary returns 200-word summary
         # business_model returns bullet points about revenue model
     ```

   - `create_company_with_analysis_context(db)`: Factory function that:
     - Creates a Company record with status='analysis'
     - Creates Page records with crawled content
     - Creates Entity records from MOCK_ENTITIES
     - Returns company object ready for analysis

   - `MockAnthropicService`: Mock class that:
     - Tracks calls with call_log list
     - Returns realistic mock responses per section
     - Simulates token usage (input: 500-1500, output: 300-800)
     - Has configurable failure mode for error testing

   - `SECTION_ORDER`: List of expected section IDs in processing order:
     `['executive_summary', 'company_overview', 'business_model', 'team_leadership', 'market_position', 'key_insights', 'red_flags']`

Use pytest fixture patterns that can be imported into test files.
  </action>
  <verify>
```bash
cd /Users/stephenhollifield/Cira && python3 -c "from backend.tests.fixtures.analysis_fixtures import MOCK_CRAWLED_CONTENT, MockAnthropicService, SECTION_ORDER; print('Fixtures import OK')"
```
  </verify>
  <done>Fixture module exists with mock content and Claude service for analysis testing.</done>
</task>

<task type="auto">
  <name>Task 2: Create analysis pipeline integration tests</name>
  <files>backend/tests/test_analysis_integration.py</files>
  <action>
Create integration tests that verify the full analysis pipeline in `backend/tests/test_analysis_integration.py`:

1. **Test class: TestClaudeAPIIntegration**
   Tests ANA-01:

   - `test_calls_claude_api_for_analysis` (ANA-01):
     - Mock AnthropicService
     - Create company with crawled content
     - Call run_full_analysis()
     - Assert AnthropicService.call() was invoked
     - Assert prompt contains company content
     - Assert system_prompt provided

   - `test_handles_api_rate_limit_with_retry`:
     - Configure mock to raise RateLimitError on first call
     - Assert retries and eventually succeeds
     - Assert delay between retries

   - `test_handles_api_timeout_gracefully`:
     - Configure mock to raise TimeoutError
     - Assert error handled, partial results preserved

2. **Test class: TestSectionGeneration**
   Tests ANA-02 through ANA-08:

   - `test_generates_executive_summary` (ANA-02):
     - Run analysis
     - Assert 'executive_summary' section in results
     - Assert content is non-empty string
     - Assert content summarizes company

   - `test_generates_company_overview` (ANA-03):
     - Run analysis
     - Assert 'company_overview' section in results
     - Assert mentions company name, founding, mission

   - `test_generates_business_model` (ANA-04):
     - Run analysis
     - Assert 'business_model' section in results
     - Assert mentions products/services

   - `test_generates_team_leadership` (ANA-05):
     - Run analysis
     - Assert 'team_leadership' section in results
     - Assert mentions key people from entities

   - `test_generates_market_position` (ANA-06):
     - Run analysis
     - Assert 'market_position' section in results

   - `test_generates_key_insights` (ANA-07):
     - Run analysis
     - Assert 'key_insights' section in results

   - `test_generates_red_flags` (ANA-08):
     - Run analysis
     - Assert 'red_flags' section in results

   - `test_generates_all_sections_in_order`:
     - Track order of section generation via progress_callback
     - Assert sections generated in SECTION_ORDER

3. **Test class: TestTokenTracking**
   Tests ANA-09:

   - `test_tracks_token_usage_per_section` (ANA-09):
     - Run analysis with mock that returns known token counts
     - Query TokenUsage table
     - Assert one record per section
     - Assert input_tokens and output_tokens recorded
     - Assert section field populated

   - `test_tracks_total_tokens_on_company`:
     - Run analysis
     - Reload company from DB
     - Assert company.total_tokens_used > 0
     - Assert matches sum of section tokens

   - `test_token_usage_includes_api_call_type`:
     - Assert all records have api_call_type='analysis'

4. **Test class: TestCostEstimation**
   Tests ANA-10:

   - `test_calculates_cost_from_token_usage` (ANA-10):
     - Run analysis with known token counts
     - Reload company from DB
     - Assert company.estimated_cost > 0
     - Assert cost calculation correct (input_rate + output_rate)

   - `test_cost_accumulates_across_sections`:
     - Run analysis
     - Assert final cost = sum of section costs

5. **Test class: TestProgressTracking**

   - `test_progress_callback_called_per_section`:
     - Provide progress_callback to run_full_analysis
     - Assert callback invoked 7 times (once per section)
     - Assert callback receives (section_id, current_index, total)

   - `test_progress_stored_in_redis`:
     - Run analysis via Celery task
     - Check Redis for progress key
     - Assert progress shows current section

6. **Test class: TestFullPipeline**

   - `test_analyze_company_end_to_end` (app fixture required):
     - Create company with pages and entities
     - Call analyze_content Celery task
     - Assert company.analysis populated
     - Assert all sections present
     - Assert token usage recorded
     - Assert company.status updated

   - `test_analysis_preserves_source_references`:
     - Run analysis
     - Assert sections reference source pages
     - Assert entity mentions linked

7. Use `@pytest.fixture` to provide mock AnthropicService via dependency injection.

8. Mock Redis for progress tracking tests.

9. Each test should be independent and use database fixtures where needed.
  </action>
  <verify>
```bash
cd /Users/stephenhollifield/Cira && python3 -m pytest backend/tests/test_analysis_integration.py -v --tb=short 2>&1 | head -80
```
  </verify>
  <done>All integration tests pass, verifying ANA-01 through ANA-10 requirements are met by existing implementation.</done>
</task>

</tasks>

<verification>
Run all integration tests to verify analysis pipeline works end-to-end:

```bash
cd /Users/stephenhollifield/Cira && python3 -m pytest backend/tests/test_analysis_integration.py -v
```

Expected: All tests pass, demonstrating requirements ANA-01 through ANA-10 are satisfied.
</verification>

<success_criteria>
- [ ] Fixture module provides mock Claude responses for each section
- [ ] Integration tests cover all 10 ANA requirements
- [ ] Tests verify component wiring (AnalysisSynthesizer -> AnthropicService -> TokenTracker)
- [ ] All tests pass with existing implementation
- [ ] Tests are documented with requirement traceability in docstrings
</success_criteria>

<output>
After completion, create `.planning/phases/03-ai-analysis/03-01-SUMMARY.md`
</output>
