{
  "id": "prd-session-1768673583084",
  "createdAt": "2026-01-17T18:13:03.084Z",
  "updatedAt": "2026-01-17T18:17:25.836Z",
  "targetVersion": 1,
  "previousVersions": [],
  "phase": "complete",
  "description": "Company Intelligence Research Assistant (CIRA) - Product Requirements Document\nVersion: 1.0\nDate: January 2026\nAuthor: Stephen\nStatus: Draft for Development\n\n1. Executive Summary\n1.1 Product Vision\nCIRA is an AI-powered company research automation tool that transforms manual due diligence into an intelligent, batch-processable workflow. It crawls company websites, analyzes content using hybrid AI (spaCy + Claude API), and generates comprehensive, actionable summaries for evaluating potential clients, partners, and vendors.\n1.2 Problem Statement\nCurrently, evaluating companies as potential clients, partners, or vendors requires hours of manual research across multiple web pages, social profiles, and external sources. The process is time-consuming, inconsistent, and prone to missing critical information.\n1.3 Solution\nAn automated research assistant that:\n\nCrawls full company websites intelligently\nExtracts structured data using spaCy NER\nGenerates contextual analysis using Claude API\nProduces consistent 2-page summaries with raw data stored for reference\nTracks token usage and processing costs\nSupports batch processing with resume capability\n\n\n2. Goals & Success Metrics\n2.1 Primary Goals\n\nReduce company research time from hours to minutes\nIncrease research comprehensiveness and consistency\nEnable batch processing of multiple companies\nProvide transparent cost tracking (token usage)\n\n2.2 Success Metrics\n\nTime savings: 80%+ reduction in manual research time\nCoverage: 95%+ capture of key company information\nReliability: Resume capability prevents data loss on interruptions\nCost transparency: Token usage tracked per company\nUser satisfaction: Useful insights in 2-page summaries\n\n\n3. User Stories\n3.1 Core User Stories\nAs a user, I want to:\n\nBatch Input\n\nUpload a CSV of companies (name, website, industry optional) to analyze multiple companies simultaneously\nSee a queue of pending analyses with status indicators\n\n\nMonitor Progress\n\nView real-time progress of each company's analysis (crawling, extracting, analyzing)\nSee time elapsed and estimated time remaining\nView token usage accumulating in real-time\n\n\nHandle Interruptions\n\nPause/resume analysis jobs without losing progress\nHave the system automatically recover if it crashes or times out\nConfigure time limits per company (variable in minutes)\n\n\nControl Analysis Depth\n\nChoose between \"Quick\" (key pages only) and \"Thorough\" (full site) modes\nSet crawl depth limits and page count maximums\nConfigure which external links to follow (LinkedIn, Twitter, etc.)\n\n\nReview Results\n\nRead a concise 2-page summary with key insights\nAccess raw data and all extracted information in the database\nSee total tokens used for the analysis\nView source links for all claims\n\n\nExport & Share\n\nExport summaries to Word, PDF, Markdown, or JSON\nRe-scan companies to detect updates/changes\nCompare previous vs. current scans\n\n\n\n\n4. Functional Requirements\n4.1 Input Management\nFR-1: Company Input Interface\n\nSingle company form with fields: Company Name, Website URL, Industry (optional)\nCSV upload for batch processing (columns: company_name, website_url, industry)\nValidation of URL format and reachability\n\nFR-2: Configuration Settings\n\nAnalysis mode: Quick / Thorough\nTime limit per company (default: 30 minutes, configurable)\nMax pages to crawl (default: 100 for thorough, 20 for quick)\nMax crawl depth (default: 3 levels)\nExternal links: Enable/disable following LinkedIn, Twitter, Facebook\nExclusion patterns (e.g., /blog/, /news/, specific domains)\n\n4.2 Web Crawling Engine\nFR-3: Intelligent Crawler\n\nParse sitemap.xml for efficient page discovery\nPrioritize key page types: About, Team, Products, Services, Contact, Careers\nImplement polite crawling (respect robots.txt, rate limiting: 1 request/second)\nHandle JavaScript-rendered content (Playwright)\nDetect and skip duplicate content\nFilter content types (skip binary files unless PDF contains text)\n\nFR-4: External Link Following\n\nExtract and follow LinkedIn company profile links\nExtract and follow Twitter/X company profile links\nExtract and follow Facebook business page links\nParse social profiles for additional company data\n\nFR-5: State Management\n\nPersist crawl state every 10 pages or 2 minutes\nStore: pages_visited, pages_queued, data_extracted, analysis_progress\nEnable resume from last checkpoint on failure/pause/timeout\nImplement timeout handling when time limit reached\n\n4.3 Data Extraction (spaCy Layer)\nFR-6: Entity Extraction\n\nExtract company information:\n\nCompany name variations\nLocations (headquarters, offices)\nFounded date\nEmployee count indicators\nRevenue/funding amounts (if mentioned)\n\n\nExtract people:\n\nLeadership team (CEO, founders, executives)\nKey employees and their roles\n\n\nExtract products/services:\n\nProduct names\nService offerings\nPricing indicators\n\n\nExtract organizations:\n\nPartners\nClients (if publicly listed)\nCompetitors (if mentioned)\nInvestors\n\n\n\nFR-7: Structured Data Capture\n\nEmail patterns (e.g., firstname@company.com)\nPhone numbers\nSocial media handles\nPhysical addresses\nTech stack indicators (from job postings, about pages)\n\n4.4 AI Analysis Layer (Claude API)\nFR-8: Content Analysis\n\nSummarize company mission and value proposition\nIdentify business model (B2B/B2C/B2B2C, SaaS, marketplace, etc.)\nAssess company stage (startup/growth/established/enterprise)\nDetect industry classification\nIdentify target market and customer segments\nExtract competitive differentiators\nIdentify potential red flags (inconsistent info, vague descriptions, etc.)\n\nFR-9: Intelligent Synthesis\n\nGenerate executive summary (3-4 paragraphs)\nCreate structured sections:\n\nCompany Overview\nBusiness Model & Products\nTeam & Leadership\nMarket Position & Differentiation\nTechnology & Operations\nKey Insights & Considerations\nRed Flags (if any)\n\n\nInclude confidence indicators for uncertain data\nCite sources for all factual claims\n\nFR-10: Token Tracking\n\nTrack tokens per Claude API call (input + output)\nAggregate total tokens used per company analysis\nDisplay cumulative token count in UI during processing\nStore final token count in database\nCalculate approximate cost based on token usage\n\n4.5 Data Storage\nFR-11: SQLite Database Schema\nCompanies Table\nsql- id (PK)\n- company_name\n- website_url\n- industry\n- analysis_mode (quick/thorough)\n- status (pending/in_progress/completed/failed/paused)\n- created_at\n- started_at\n- completed_at\n- time_limit_minutes\n- total_tokens_used\n- estimated_cost\n- last_checkpoint\nCrawl_Sessions Table\nsql- id (PK)\n- company_id (FK)\n- pages_crawled\n- pages_queued\n- crawl_depth_reached\n- external_links_followed\n- status (active/paused/completed/timeout)\n- checkpoint_data (JSON)\nPages Table\nsql- id (PK)\n- company_id (FK)\n- url\n- page_type (about/team/product/contact/other)\n- content_hash\n- raw_html\n- extracted_text\n- crawled_at\n- is_external\nEntities Table\nsql- id (PK)\n- company_id (FK)\n- entity_type (person/org/location/product/date/money)\n- entity_value\n- context_snippet\n- source_url\n- confidence_score\nAnalyses Table\nsql- id (PK)\n- company_id (FK)\n- version_number\n- executive_summary\n- full_analysis (JSON with sections)\n- raw_insights (JSON)\n- token_breakdown (JSON)\n- created_at\nToken_Usage Table\nsql- id (PK)\n- company_id (FK)\n- api_call_type (extraction/summarization/analysis)\n- input_tokens\n- output_tokens\n- timestamp\n4.6 Output Generation\nFR-12: Summary Document Structure\nGenerate 2-page markdown with:\nmarkdown# [Company Name] - Intelligence Brief\n\n**Analysis Date:** [timestamp]\n**Website:** [url]\n**Industry:** [industry]\n**Analysis Mode:** [quick/thorough]\n**Total Tokens Used:** [count]\n\n## Executive Summary\n[3-4 paragraph overview]\n\n## Company Overview\n- **Founded:** [year or \"Not disclosed\"]\n- **Headquarters:** [location]\n- **Company Size:** [employee range]\n- **Stage:** [startup/growth/established]\n\n## Business Model & Products\n[Description of what they do, how they make money]\n\n## Team & Leadership\n[Key executives and team insights]\n\n## Market Position & Differentiation\n[Unique value props, competitive advantages]\n\n## Technology & Operations\n[Tech stack, operational insights]\n\n## Key Insights & Considerations\n[Strategic insights for evaluation]\n\n## Red Flags & Considerations\n[Any concerns or uncertainties]\n\n---\n*Sources: [X pages analyzed] | [Links to key pages]*\n```\n\n**FR-13: Export Formats**\n- **Markdown**: Native format (UTF-8)\n- **Word (.docx)**: Using python-docx with formatting\n- **PDF**: Using ReportLab or weasyprint\n- **JSON**: Structured data export with all fields\n\n### 4.7 Re-scan & Comparison\n\n**FR-14: Update Detection**\n- Store content hashes for key pages\n- Enable \"Re-scan\" button for any completed analysis\n- Compare new scan against previous version\n- Highlight changes in: team, products, content\n- Store version history (keep last 3 scans)\n\n---\n\n## 5. Technical Architecture\n\n### 5.1 Technology Stack\n\n**Frontend**\n- React 18+ with TypeScript\n- Vite for build tooling\n- TanStack Query for data fetching\n- Tailwind CSS for styling\n- Recharts for token usage visualization\n\n**Backend**\n- Python 3.11+\n- Flask 3.0+ for REST API\n- SQLAlchemy for ORM\n- Celery for background job processing\n- Redis for Celery broker and caching\n\n**Data Processing**\n- Playwright for web crawling (handles JS)\n- BeautifulSoup4 for HTML parsing\n- spaCy 3.7+ with en_core_web_lg model\n- Anthropic Python SDK for Claude API\n- python-docx for Word export\n- ReportLab for PDF export\n\n**Storage**\n- SQLite (development and production - suitable for single user)\n- Option to migrate to PostgreSQL later if needed\n\n### 5.2 System Architecture\n```\n┌─────────────────────────────────────────────────────────┐\n│                    React Frontend                        │\n│  - Batch Input Form     - Progress Dashboard            │\n│  - Company Queue        - Results Viewer                │\n│  - Export Controls      - Settings Panel                │\n└─────────────────────┬───────────────────────────────────┘\n                      │ REST API\n┌─────────────────────┴───────────────────────────────────┐\n│                    Flask Backend                         │\n│  - API Routes       - Job Management                    │\n│  - Auth             - State Persistence                 │\n└─────────────────────┬───────────────────────────────────┘\n                      │\n┌─────────────────────┴───────────────────────────────────┐\n│                  Celery Workers                          │\n│  ┌─────────────────────────────────────────────────┐   │\n│  │  Crawl Worker (Job 1)                           │   │\n│  │  - Playwright crawler → Pages Queue             │   │\n│  │  - Checkpoint every 10 pages                    │   │\n│  │  - Timeout monitoring                           │   │\n│  └─────────────────────────────────────────────────┘   │\n│  ┌─────────────────────────────────────────────────┐   │\n│  │  Extract Worker (Job 2)                         │   │\n│  │  - spaCy NER → Entities Table                   │   │\n│  │  - Structured data extraction                   │   │\n│  └─────────────────────────────────────────────────┘   │\n│  ┌─────────────────────────────────────────────────┐   │\n│  │  Analyze Worker (Job 3)                         │   │\n│  │  - Claude API calls → Analysis                  │   │\n│  │  - Token tracking                               │   │\n│  │  - Summary generation                           │   │\n│  └─────────────────────────────────────────────────┘   │\n└─────────────────────┬───────────────────────────────────┘\n                      │\n┌─────────────────────┴───────────────────────────────────┐\n│                   Data Layer                             │\n│  - SQLite Database  - File Storage                      │\n│  - Redis Cache      - Export Files                      │\n└──────────────────────────────────────────────────────────┘\n```\n\n### 5.3 Job Processing Flow\n```\n1. User submits batch → Creates company records (status: pending)\n2. For each company:\n   a. Crawl Worker starts → status: in_progress\n      - Discover pages via sitemap + breadth-first crawl\n      - Checkpoint every 10 pages or 2 minutes\n      - Follow external links if configured\n      - Stop at time limit or max pages\n      \n   b. Extract Worker processes pages\n      - spaCy NER on all page text\n      - Store entities in database\n      - Extract structured data (emails, phones, etc.)\n      \n   c. Analyze Worker synthesizes insights\n      - Prepare context from entities + page content\n      - Call Claude API for analysis sections\n      - Track tokens per call\n      - Generate final summary\n      - status: completed\n      \n3. User views results → Can export or re-scan\n5.4 State Management for Resume Capability\nCheckpoint Data Structure (JSON)\njson{\n  \"pages_visited\": [\"url1\", \"url2\", ...],\n  \"pages_queued\": [\"url3\", \"url4\", ...],\n  \"external_links_found\": [\"linkedin_url\", \"twitter_url\"],\n  \"current_depth\": 2,\n  \"crawl_start_time\": \"ISO timestamp\",\n  \"last_checkpoint_time\": \"ISO timestamp\",\n  \"entities_extracted_count\": 45,\n  \"analysis_sections_completed\": [\"overview\", \"business_model\"]\n}\nResume Logic\n\nOn startup, check for in_progress or paused jobs\nLoad checkpoint_data from database\nSkip already-visited URLs\nContinue from pages_queued\nResume analysis from last completed section\n\n\n6. API Specifications\n6.1 REST Endpoints\nPOST /api/companies\n\nCreate single company analysis job\nBody: {company_name, website_url, industry?, config?}\nReturns: {company_id, status}\n\nPOST /api/companies/batch\n\nUpload CSV for batch processing\nBody: FormData with CSV file\nReturns: {job_ids: [...], total_count}\n\nGET /api/companies\n\nList all companies with status\nQuery params: ?status=completed&sort=created_at&order=desc\nReturns: [{company_id, name, status, tokens_used, created_at}, ...]\n\nGET /api/companies/:id\n\nGet company details and analysis\nReturns: {company, analysis, entities, token_usage}\n\nGET /api/companies/:id/progress\n\nReal-time progress for in-progress jobs\nReturns: {status, pages_crawled, pages_total, current_phase, tokens_used, time_elapsed}\n\nPOST /api/companies/:id/pause\n\nPause an in-progress analysis\nReturns: {status: \"paused\", checkpoint_saved: true}\n\nPOST /api/companies/:id/resume\n\nResume a paused analysis\nReturns: {status: \"in_progress\", resumed_from: checkpoint}\n\nPOST /api/companies/:id/rescan\n\nInitiate re-scan for updates\nReturns: {new_analysis_id, version_number}\n\nGET /api/companies/:id/export\n\nExport analysis in specified format\nQuery params: ?format=markdown|word|pdf|json\nReturns: File download\n\nGET /api/companies/:id/tokens\n\nDetailed token usage breakdown\nReturns: {total_tokens, by_api_call: [...], estimated_cost}\n\nDELETE /api/companies/:id\n\nDelete company and all associated data\nReturns: {deleted: true}\n\n\n7. UI/UX Requirements\n7.1 Main Views\n1. Home/Dashboard\n\nCompany list table with columns:\n\nCompany Name\nWebsite\nStatus (pending/in_progress/completed/failed)\nProgress bar (for in-progress)\nTokens Used\nDate\nActions (View/Export/Re-scan/Delete)\n\n\nFilters: Status, Date range\nBatch upload button\nAdd single company button\n\n2. Add Company Form\n\nCompany Name (required)\nWebsite URL (required, validated)\nIndustry (optional dropdown with custom option)\nAdvanced settings (collapsible):\n\nAnalysis Mode: Quick / Thorough\nTime Limit (minutes): slider 5-120\nMax Pages: input field\nMax Depth: 1-5\nFollow External Links: checkboxes (LinkedIn, Twitter, Facebook)\nExclusion Patterns: text area\n\n\nSubmit button\n\n3. Progress View (Live Updates)\n\nCompany name + website at top\nReal-time status: \"Crawling...\", \"Extracting entities...\", \"Analyzing...\"\nProgress bar with percentage\nMetrics dashboard:\n\nPages crawled: X / Y\nEntities extracted: count\nTokens used: live counter\nTime elapsed / Time remaining\n\n\nLogs/activity feed (last 10 actions)\nPause/Cancel buttons\n\n4. Results View\n\nTwo-column layout:\n\nLeft: Summary (markdown rendered)\nRight: Metadata sidebar\n\nToken usage (total + breakdown)\nEstimated cost\nAnalysis date\nVersion history\nSource links\n\n\n\n\nAction buttons:\n\nExport (dropdown: Word/PDF/Markdown/JSON)\nRe-scan\nView Raw Data (modal with tables)\nDelete\n\n\nTabs:\n\nSummary\nEntities (filterable table)\nPages Analyzed (list with links)\nToken Usage (chart visualization)\n\n\n\n5. Batch Upload\n\nDrag-and-drop CSV or file picker\nCSV template download button\nPreview table of parsed companies\nValidation messages\nSubmit batch button\n\n7.2 Visual Design\nColor Scheme\n\nPrimary: Blue (#2563eb) for actions\nSuccess: Green (#10b981) for completed\nWarning: Yellow (#f59e0b) for in-progress\nError: Red (#ef4444) for failed\nNeutral: Gray shades for backgrounds\n\nTypography\n\nHeadings: Inter or system-ui\nBody: System font stack for readability\nCode/Data: Monospace font\n\nComponents\n\nMaterial-inspired design with shadows\nLoading skeletons for async data\nToast notifications for actions\nConfirmation modals for destructive actions\n\n\n8. Implementation Phases\nPhase 1: Core Infrastructure (Week 1-2)\n\n Set up project structure (React + Flask)\n Database schema and SQLAlchemy models\n Basic REST API with CRUD operations\n Celery setup with Redis broker\n Basic React UI shell with routing\n\nPhase 2: Crawling Engine (Week 2-3)\n\n Playwright crawler implementation\n Sitemap parsing and priority detection\n Rate limiting and polite crawling\n State persistence and checkpoint system\n Timeout handling\n External link following logic\n\nPhase 3: Data Extraction (Week 3-4)\n\n spaCy integration and model setup\n Entity extraction pipeline\n Structured data extraction (emails, phones, etc.)\n Data storage in entities table\n Duplicate detection\n\nPhase 4: AI Analysis (Week 4-5)\n\n Claude API integration\n Prompt engineering for sections\n Token tracking implementation\n Analysis synthesis and summary generation\n Confidence scoring\n\nPhase 5: UI Development (Week 5-6)\n\n Company list dashboard\n Add company form with validation\n Progress tracking view (live updates)\n Results viewer with markdown rendering\n Token usage visualization\n\nPhase 6: Export & Resume (Week 6-7)\n\n Markdown export (native)\n Word export (python-docx)\n PDF export (ReportLab)\n JSON export\n Pause/resume functionality\n Re-scan and version comparison\n\nPhase 7: Polish & Testing (Week 7-8)\n\n Error handling and user feedback\n Performance optimization\n Batch processing testing\n Documentation (user guide + API docs)\n Deployment setup\n\n\n9. Non-Functional Requirements\n9.1 Performance\n\nCrawl speed: 1-2 pages/second (respecting rate limits)\nUI responsiveness: <200ms for user interactions\nDatabase queries: <100ms for most operations\nExport generation: <5 seconds for standard formats\n\n9.2 Reliability\n\n99% uptime for user-facing components\nAutomatic retry on transient failures (network errors)\nGraceful degradation if Claude API unavailable\nData integrity with transaction management\n\n9.3 Security\n\nInput validation on all user inputs\nSQL injection prevention (parameterized queries)\nRate limiting on API endpoints\nSecure API key storage (environment variables)\nNo storage of sensitive scraped data (passwords, PII beyond business info)\n\n9.4 Scalability\n\nSupport for 100+ companies in batch\nConcurrent analysis jobs (configurable worker count)\nDatabase optimized for 10,000+ companies\nEfficient pagination for large datasets\n\n9.5 Usability\n\nIntuitive UI requiring no training\nClear error messages with actionable guidance\nConsistent visual feedback for all operations\nResponsive design (desktop-first, but mobile-friendly)\n\n\n10. Configuration & Environment\n10.1 Environment Variables\nbash# Flask\nFLASK_APP=app.py\nFLASK_ENV=development\nSECRET_KEY=<random-key>\n\n# Database\nDATABASE_URL=sqlite:///cira.db\n\n# Celery\nCELERY_BROKER_URL=redis://localhost:6379/0\nCELERY_RESULT_BACKEND=redis://localhost:6379/0\n\n# APIs\nANTHROPIC_API_KEY=<your-api-key>\n\n# Crawling\nDEFAULT_TIME_LIMIT_MINUTES=30\nDEFAULT_MAX_PAGES=100\nDEFAULT_MAX_DEPTH=3\nCRAWL_RATE_LIMIT_SECONDS=1\n\n# spaCy\nSPACY_MODEL=en_core_web_lg\n10.2 Configuration File (config.yaml)\nyamlanalysis:\n  quick_mode:\n    max_pages: 20\n    max_depth: 2\n    follow_external: false\n  thorough_mode:\n    max_pages: 100\n    max_depth: 3\n    follow_external: true\n\ncrawling:\n  user_agent: \"CIRA Bot/1.0\"\n  timeout_seconds: 30\n  retry_attempts: 3\n  \nextraction:\n  min_text_length: 50\n  entity_confidence_threshold: 0.6\n\nanalysis:\n  summary_max_length: 2000  # characters\n  section_word_limits:\n    executive_summary: 300\n    overview: 200\n    business_model: 250\n    team: 150\n    market_position: 200\n    technology: 150\n    insights: 200\n\n11. Future Enhancements (Out of Scope for V1)\n\nIntegration Features\n\nSalesforce/CRM integration\nSlack notifications on completion\nWebhook support for external systems\n\n\nAdvanced Analysis\n\nSentiment analysis on company communications\nCompetitive landscape mapping\nFinancial health indicators (from public filings)\nSocial media sentiment aggregation\n\n\nCollaboration\n\nMulti-user support with permissions\nShared workspaces\nComments and annotations on analyses\nTeam activity feed\n\n\nIntelligence\n\nML model to predict good fit (client/partner/vendor)\nAutomated scoring system\nTrend detection across multiple scans\nIndustry benchmarking\n\n\nEnhanced Outputs\n\nCustom report templates\nPowerPoint export\nInteractive dashboard with charts\nComparison reports (Company A vs. Company B)\n\n\n\n\n12. Risks & Mitigation\nRiskImpactMitigationWebsite blocks crawlerHighImplement rotating user agents, respect robots.txt, add delaysClaude API rate limitsMediumImplement exponential backoff, queue managementIncomplete data extractionMediumConfidence scoring, manual review flagLarge sites exceed time limitsMediumSmart prioritization of key pages, checkpoint systemCost overruns from token usageLowPre-analysis cost estimates, usage alertsDatabase size growthLowImplement data retention policies, archiving\n\n13. Success Criteria\nMVP is successful if:\n\nCan analyze 10 companies in batch with 90%+ success rate\nGenerates useful 2-page summaries in under 30 minutes each\nResume capability works 100% of the time after interruptions\nToken tracking is accurate within 5% margin\nExport formats are properly formatted and readable\nUser can complete full workflow without confusion\n\nLong-term success:\n\nRegular usage (10+ companies/month)\nTime savings validated (80%+ reduction vs. manual)\nSummaries inform actual business decisions\nUsers request additional features\n\n\n14. Appendices\nA. Sample CSV Format\ncsvcompany_name,website_url,industry\nAcme Corp,https://www.acmecorp.com,Technology\nExample Inc,https://example.com,Manufacturing\nB. Sample Summary Output\nSee FR-12 for detailed structure.\nC. Token Cost Estimation\n\nQuick Mode: 50K-150K tokens (~$0.15-$0.45 per company)\nThorough Mode: 200K-500K tokens (~$0.60-$1.50 per company)\nBased on Claude Sonnet pricing (as of Jan 2026)\n\nD. Development Tools Recommendation\n\nIDE: Cursor with GitHub Copilot\nVersion Control: Git + GitHub\nProject Management: Linear or GitHub Projects\nTesting: Pytest (backend), Vitest (frontend)\nCI/CD: GitHub Actions\nDeployment: Docker containers, Railway/Render for hosting\n\n\nEnd of PRD",
  "contextDocs": [
    "IMPLEMENTATION_PLAN.md"
  ],
  "rounds": [],
  "finalPrd": "# **CIRA - Company Intelligence Research Assistant**\n\n## **Product Requirements Document for AI Development**\n\n---\n\n| Field | Value |\n| ----- | ----- |\n| **Version** | 1 |\n| **Date** | January 17, 2026 |\n| **Status** | Draft |\n\n---\n\n## **Table of Contents**\n\n1. [Project Overview](#1-project-overview)\n2. [Technical Architecture](#2-technical-architecture)\n3. [Data Models & Interfaces](#3-data-models--interfaces)\n4. [Functional Requirements](#4-functional-requirements)\n5. [User Roles & Access Control](#5-user-roles--access-control)\n6. [User Interface Specifications](#6-user-interface-specifications)\n7. [API Specifications](#7-api-specifications)\n8. [Integration Requirements](#8-integration-requirements)\n9. [Non-Functional Requirements](#9-non-functional-requirements)\n10. [Glossary](#10-glossary)\n\n---\n\n## **1. Project Overview**\n\n### **1.1 Purpose**\n\nCIRA (Company Intelligence Research Assistant) is an AI-powered company research automation tool that transforms manual due diligence into an intelligent, batch-processable workflow. The system crawls company websites, analyzes content using hybrid AI (spaCy for NER + Claude API for contextual analysis), and generates comprehensive, actionable summaries for evaluating potential clients, partners, and vendors.\n\nThe core problem CIRA solves is the time-intensive nature of company research. Currently, evaluating a single company requires hours of manual research across multiple web pages, social profiles, and external sources. This process is inconsistent, prone to missing critical information, and doesn't scale well when evaluating multiple companies simultaneously.\n\n### **1.2 Scope**\n\n**In Scope:**\n- Web crawling engine with intelligent page prioritization\n- Entity extraction using spaCy NLP\n- AI-powered analysis and synthesis using Claude API\n- Batch processing with pause/resume capability\n- Token usage tracking and cost estimation\n- Multiple export formats (Markdown, Word, PDF, JSON)\n- Re-scan functionality with change detection\n- SQLite-based data persistence\n\n**Out of Scope (V1):**\n- Multi-user collaboration features\n- CRM/Salesforce integration\n- Webhook notifications\n- ML-based company fit prediction\n- Competitive landscape mapping\n- Financial health analysis from public filings\n- Custom report templates\n- Mobile native applications\n\n### **1.3 Key Features Summary**\n\n| Feature | Description | Priority |\n| ------- | ----------- | -------- |\n| Intelligent Web Crawler | Crawls company websites with sitemap parsing, JavaScript rendering, and polite crawling | P0 |\n| Entity Extraction | spaCy-powered NER for extracting people, organizations, locations, products, and key data | P0 |\n| AI Analysis | Claude API integration for contextual analysis, business model identification, and summary generation | P0 |\n| Batch Processing | CSV upload for analyzing multiple companies with queue management | P0 |\n| Progress Monitoring | Real-time progress tracking with pages crawled, tokens used, and time elapsed | P0 |\n| Checkpoint & Resume | State persistence enabling pause/resume without data loss | P0 |\n| Token Tracking | Per-company and per-call token usage tracking with cost estimation | P0 |\n| Export Generation | 2-page summary export to Markdown, Word, PDF, and JSON formats | P1 |\n| External Link Following | Optional crawling of LinkedIn, Twitter, and Facebook company profiles | P1 |\n| Re-scan & Comparison | Detect changes between scans with version history | P1 |\n| Configuration Options | Quick vs Thorough modes with customizable crawl depth, page limits, and time limits | P1 |\n\n### **1.4 System Context Diagram**\n\n```\n                                    ┌─────────────────┐\n                                    │   User/Browser  │\n                                    └────────┬────────┘\n                                             │ HTTPS\n                                             ▼\n┌────────────────────────────────────────────────────────────────────────────┐\n│                              CIRA System                                    │\n│  ┌─────────────────────────────────────────────────────────────────────┐  │\n│  │                        React Frontend                                │  │\n│  │   Dashboard │ Company Form │ Progress View │ Results │ Export       │  │\n│  └──────────────────────────────┬──────────────────────────────────────┘  │\n│                                 │ REST API                                 │\n│  ┌──────────────────────────────┴──────────────────────────────────────┐  │\n│  │                         Flask Backend                                │  │\n│  │              API Routes │ Job Management │ State Persistence         │  │\n│  └──────────────────────────────┬──────────────────────────────────────┘  │\n│                                 │                                          │\n│  ┌──────────────────────────────┴──────────────────────────────────────┐  │\n│  │                       Celery Workers                                 │  │\n│  │     Crawl Worker  │  Extract Worker  │  Analyze Worker              │  │\n│  └──────────────────────────────┬──────────────────────────────────────┘  │\n│                                 │                                          │\n│  ┌──────────────────────────────┴──────────────────────────────────────┐  │\n│  │                        Data Layer                                    │  │\n│  │            SQLite Database  │  Redis Cache  │  File Storage         │  │\n│  └─────────────────────────────────────────────────────────────────────┘  │\n└────────────────────────────────────────────────────────────────────────────┘\n         │                    │                         │\n         ▼                    ▼                         ▼\n┌─────────────┐    ┌──────────────────┐    ┌─────────────────────┐\n│ Target      │    │ Claude API       │    │ Social Platforms    │\n│ Company     │    │ (Anthropic)      │    │ (LinkedIn, Twitter, │\n│ Websites    │    │                  │    │  Facebook)          │\n└─────────────┘    └──────────────────┘    └─────────────────────┘\n```\n\n---\n\n## **2. Technical Architecture**\n\n### **2.1 Technology Stack**\n\n#### **Frontend**\n\n| Technology | Version | Purpose |\n| ---------- | ------- | ------- |\n| React | 18+ | Component-based UI framework |\n| TypeScript | 5.0+ | Type-safe JavaScript |\n| Vite | 5.0+ | Build tooling and dev server |\n| TanStack Query | 5.0+ | Server state management and caching |\n| Tailwind CSS | 3.4+ | Utility-first styling |\n| Recharts | 2.10+ | Token usage visualization charts |\n| React Router | 6.0+ | Client-side routing |\n| Axios | 1.6+ | HTTP client |\n\n#### **Backend**\n\n| Technology | Version | Purpose |\n| ---------- | ------- | ------- |\n| Python | 3.11+ | Runtime environment |\n| Flask | 3.0+ | REST API framework |\n| SQLAlchemy | 2.0+ | ORM and database abstraction |\n| Celery | 5.3+ | Distributed task queue |\n| Redis | 7.0+ | Message broker and caching |\n| Playwright | 1.40+ | JavaScript-rendered page crawling |\n| BeautifulSoup4 | 4.12+ | HTML parsing |\n| spaCy | 3.7+ | NLP and entity extraction |\n| Anthropic SDK | 0.18+ | Claude API integration |\n| python-docx | 1.1+ | Word document generation |\n| ReportLab | 4.0+ | PDF generation |\n| Pydantic | 2.5+ | Data validation |\n\n#### **Infrastructure**\n\n| Category | Services |\n| -------- | -------- |\n| Database | SQLite (primary), Redis (cache/broker) |\n| Task Queue | Celery with Redis broker |\n| File Storage | Local filesystem for exports |\n| Containerization | Docker, Docker Compose |\n| Deployment | Railway, Render, or self-hosted |\n\n### **2.2 Worker Architecture**\n\n| Worker | Responsibility |\n| ------- | -------------- |\n| Crawl Worker | Discovers and fetches web pages, handles sitemap parsing, manages crawl state |\n| Extract Worker | Runs spaCy NER pipeline, extracts structured data (emails, phones, addresses) |\n| Analyze Worker | Calls Claude API, synthesizes insights, generates summaries, tracks tokens |\n\n#### **Inter-Service Communication**\n\n| Type | Technology | Use Case |\n| ---- | ---------- | -------- |\n| Task Queue | Celery + Redis | Worker job distribution |\n| REST API | Flask | Frontend-backend communication |\n| Cache | Redis | Progress state, session data |\n| Database | SQLite | Persistent storage |\n\n### **2.3 Database Architecture**\n\n| Database | Purpose | Data Types |\n| -------- | ------- | ---------- |\n| SQLite | Primary storage | Companies, crawl sessions, pages, entities, analyses, token usage |\n| Redis | Caching and message broker | Job queues, progress state, session cache |\n\n---\n\n## **3. Data Models & Interfaces**\n\n### **3.1 Core Entities**\n\n```typescript\ninterface Company {\n  id: string;\n  companyName: string;\n  websiteUrl: string;\n  industry: string | null;\n  analysisMode: AnalysisMode;\n  status: CompanyStatus;\n  createdAt: Date;\n  startedAt: Date | null;\n  completedAt: Date | null;\n  timeLimitMinutes: number;\n  totalTokensUsed: number;\n  estimatedCost: number;\n  lastCheckpoint: string | null;\n}\n\ninterface CrawlSession {\n  id: string;\n  companyId: string;\n  pagesCrawled: number;\n  pagesQueued: number;\n  crawlDepthReached: number;\n  externalLinksFollowed: number;\n  status: CrawlStatus;\n  checkpointData: CheckpointData;\n  createdAt: Date;\n  updatedAt: Date;\n}\n\ninterface Page {\n  id: string;\n  companyId: string;\n  url: string;\n  pageType: PageType;\n  contentHash: string;\n  rawHtml: string;\n  extractedText: string;\n  crawledAt: Date;\n  isExternal: boolean;\n}\n\ninterface Entity {\n  id: string;\n  companyId: string;\n  entityType: EntityType;\n  entityValue: string;\n  contextSnippet: string;\n  sourceUrl: string;\n  confidenceScore: number;\n  createdAt: Date;\n}\n\ninterface Analysis {\n  id: string;\n  companyId: string;\n  versionNumber: number;\n  executiveSummary: string;\n  fullAnalysis: AnalysisSections;\n  rawInsights: Record<string, any>;\n  tokenBreakdown: TokenBreakdown;\n  createdAt: Date;\n}\n\ninterface TokenUsage {\n  id: string;\n  companyId: string;\n  apiCallType: ApiCallType;\n  inputTokens: number;\n  outputTokens: number;\n  timestamp: Date;\n}\n\ninterface CheckpointData {\n  pagesVisited: string[];\n  pagesQueued: string[];\n  externalLinksFound: string[];\n  currentDepth: number;\n  crawlStartTime: string;\n  lastCheckpointTime: string;\n  entitiesExtractedCount: number;\n  analysisSectionsCompleted: string[];\n}\n\ninterface AnalysisSections {\n  companyOverview: SectionContent;\n  businessModelProducts: SectionContent;\n  teamLeadership: SectionContent;\n  marketPosition: SectionContent;\n  technologyOperations: SectionContent;\n  keyInsights: SectionContent;\n  redFlags: SectionContent | null;\n}\n\ninterface SectionContent {\n  content: string;\n  sources: string[];\n  confidence: number;\n}\n\ninterface TokenBreakdown {\n  totalInputTokens: number;\n  totalOutputTokens: number;\n  bySection: Record<string, { input: number; output: number }>;\n}\n\ninterface AnalysisConfig {\n  analysisMode: AnalysisMode;\n  timeLimitMinutes: number;\n  maxPages: number;\n  maxDepth: number;\n  followLinkedIn: boolean;\n  followTwitter: boolean;\n  followFacebook: boolean;\n  exclusionPatterns: string[];\n}\n```\n\n### **3.2 Common Types**\n\n```typescript\ninterface AuditInfo {\n  createdAt: Date;\n  createdBy: string;\n  updatedAt: Date;\n  updatedBy: string;\n  version: number;\n}\n\ninterface PaginatedResponse<T> {\n  data: T[];\n  meta: {\n    total: number;\n    page: number;\n    pageSize: number;\n    totalPages: number;\n  };\n}\n\ninterface ApiResponse<T> {\n  success: boolean;\n  data?: T;\n  error?: {\n    code: string;\n    message: string;\n    details?: Record<string, any>;\n  };\n}\n\ninterface ProgressUpdate {\n  companyId: string;\n  status: CompanyStatus;\n  phase: ProcessingPhase;\n  pagesCrawled: number;\n  pagesTotal: number | null;\n  entitiesExtracted: number;\n  tokensUsed: number;\n  timeElapsed: number;\n  estimatedTimeRemaining: number | null;\n  currentActivity: string;\n}\n\ninterface ExportOptions {\n  format: ExportFormat;\n  includeRawData: boolean;\n  includeSources: boolean;\n}\n\ninterface BatchUploadResult {\n  successful: number;\n  failed: number;\n  companies: Array<{\n    companyName: string;\n    companyId: string | null;\n    error: string | null;\n  }>;\n}\n\ninterface ComparisonResult {\n  companyId: string;\n  previousVersion: number;\n  currentVersion: number;\n  changes: {\n    team: ChangeDetail[];\n    products: ChangeDetail[];\n    content: ChangeDetail[];\n  };\n  significantChanges: boolean;\n}\n\ninterface ChangeDetail {\n  field: string;\n  previousValue: string | null;\n  currentValue: string | null;\n  changeType: 'added' | 'removed' | 'modified';\n}\n```\n\n### **3.3 Enums**\n\n```typescript\nenum CompanyStatus {\n  PENDING = 'pending',\n  IN_PROGRESS = 'in_progress',\n  COMPLETED = 'completed',\n  FAILED = 'failed',\n  PAUSED = 'paused'\n}\n\nenum CrawlStatus {\n  ACTIVE = 'active',\n  PAUSED = 'paused',\n  COMPLETED = 'completed',\n  TIMEOUT = 'timeout'\n}\n\nenum AnalysisMode {\n  QUICK = 'quick',\n  THOROUGH = 'thorough'\n}\n\nenum PageType {\n  ABOUT = 'about',\n  TEAM = 'team',\n  PRODUCT = 'product',\n  SERVICE = 'service',\n  CONTACT = 'contact',\n  CAREERS = 'careers',\n  PRICING = 'pricing',\n  BLOG = 'blog',\n  NEWS = 'news',\n  OTHER = 'other'\n}\n\nenum EntityType {\n  PERSON = 'person',\n  ORGANIZATION = 'org',\n  LOCATION = 'location',\n  PRODUCT = 'product',\n  DATE = 'date',\n  MONEY = 'money',\n  EMAIL = 'email',\n  PHONE = 'phone',\n  ADDRESS = 'address',\n  SOCIAL_HANDLE = 'social_handle',\n  TECH_STACK = 'tech_stack'\n}\n\nenum ApiCallType {\n  EXTRACTION = 'extraction',\n  SUMMARIZATION = 'summarization',\n  ANALYSIS = 'analysis'\n}\n\nenum ExportFormat {\n  MARKDOWN = 'markdown',\n  WORD = 'word',\n  PDF = 'pdf',\n  JSON = 'json'\n}\n\nenum ProcessingPhase {\n  QUEUED = 'queued',\n  CRAWLING = 'crawling',\n  EXTRACTING = 'extracting',\n  ANALYZING = 'analyzing',\n  GENERATING = 'generating',\n  COMPLETED = 'completed'\n}\n```\n\n---\n\n## **4. Functional Requirements**\n\n### **4.1 Company Input Management**\n\n#### **4.1.1 Single Company Input**\n\n| ID | Requirement | Priority | Acceptance Criteria |\n| -- | ----------- | -------- | ------------------- |\n| FR-INP-001 | System shall provide a form to add a single company with name, website URL, and optional industry | P0 | Form renders with all fields; validation prevents submission without required fields |\n| FR-INP-002 | System shall validate URL format and check reachability before accepting | P0 | Invalid URLs show error message; unreachable sites show warning with option to proceed |\n| FR-INP-003 | System shall provide industry selection via dropdown with custom option | P1 | Dropdown shows common industries; custom text input available |\n| FR-INP-004 | System shall expose advanced configuration options (mode, time limit, max pages, crawl depth, external links, exclusions) | P1 | All config options functional; defaults applied when not specified |\n\n#### **4.1.2 Batch Upload**\n\n| ID | Requirement | Priority | Acceptance Criteria |\n| -- | ----------- | -------- | ------------------- |\n| FR-BAT-001 | System shall accept CSV file upload with columns: company_name, website_url, industry | P0 | CSV parsed correctly; preview shown before submission |\n| FR-BAT-002 | System shall provide downloadable CSV template | P0 | Template download link works; template has correct headers |\n| FR-BAT-003 | System shall validate all rows before processing and show errors per row | P0 | Invalid rows highlighted; valid rows can proceed while skipping invalid |\n| FR-BAT-004 | System shall create analysis jobs for all valid companies in batch | P0 | All valid companies appear in queue with pending status |\n\n### **4.2 Web Crawling Engine**\n\n#### **4.2.1 Page Discovery**\n\n| ID | Requirement | Priority | Acceptance Criteria |\n| -- | ----------- | -------- | ------------------- |\n| FR-CRL-001 | System shall parse sitemap.xml when available for efficient page discovery | P0 | Sitemap detected and parsed; pages added to queue |\n| FR-CRL-002 | System shall prioritize key page types: About, Team, Products, Services, Contact, Careers | P0 | Priority pages crawled first regardless of discovery order |\n| FR-CRL-003 | System shall implement breadth-first crawling with configurable depth limit | P0 | Crawl respects depth limit; pages discovered in BFS order |\n| FR-CRL-004 | System shall detect and skip duplicate content using content hashing | P1 | Duplicate pages not re-processed; hash stored for comparison |\n\n#### **4.2.2 Crawl Behavior**\n\n| ID | Requirement | Priority | Acceptance Criteria |\n| -- | ----------- | -------- | ------------------- |\n| FR-CRL-005 | System shall respect robots.txt directives | P0 | Disallowed paths not crawled |\n| FR-CRL-006 | System shall implement rate limiting (1 request/second default) | P0 | Request timing verified; no parallel requests to same domain |\n| FR-CRL-007 | System shall handle JavaScript-rendered content using Playwright | P0 | SPA pages render fully before extraction |\n| FR-CRL-008 | System shall skip binary files except text-containing PDFs | P1 | Images, videos ignored; PDFs with text extracted |\n| FR-CRL-009 | System shall stop crawling when time limit reached | P0 | Crawl stops at time limit; checkpoint saved |\n| FR-CRL-010 | System shall stop crawling when max page limit reached | P0 | Crawl stops at page limit; final state persisted |\n\n#### **4.2.3 External Link Following**\n\n| ID | Requirement | Priority | Acceptance Criteria |\n| -- | ----------- | -------- | ------------------- |\n| FR-EXT-001 | System shall extract and optionally follow LinkedIn company profile links | P1 | LinkedIn URLs detected; profile data extracted when enabled |\n| FR-EXT-002 | System shall extract and optionally follow Twitter/X company profile links | P1 | Twitter URLs detected; profile data extracted when enabled |\n| FR-EXT-003 | System shall extract and optionally follow Facebook business page links | P1 | Facebook URLs detected; profile data extracted when enabled |\n| FR-EXT-004 | System shall mark external pages distinctly in storage | P1 | is_external flag set correctly for all external pages |\n\n### **4.3 State Management & Resume**\n\n#### **4.3.1 Checkpointing**\n\n| ID | Requirement | Priority | Acceptance Criteria |\n| -- | ----------- | -------- | ------------------- |\n| FR-STA-001 | System shall persist crawl state every 10 pages or 2 minutes | P0 | Checkpoint written at defined intervals; data recoverable |\n| FR-STA-002 | Checkpoint shall store: pages_visited, pages_queued, external_links_found, current_depth, timestamps, extraction counts, analysis progress | P0 | All checkpoint fields populated and accurate |\n| FR-STA-003 | System shall allow user to pause an in-progress analysis | P0 | Pause button functional; status changes to paused; checkpoint saved |\n| FR-STA-004 | System shall allow user to resume a paused analysis | P0 | Resume continues from last checkpoint; no duplicate work |\n\n#### **4.3.2 Recovery**\n\n| ID | Requirement | Priority | Acceptance Criteria |\n| -- | ----------- | -------- | ------------------- |\n| FR-STA-005 | System shall automatically resume in_progress jobs on startup | P0 | Jobs resume after server restart; progress not lost |\n| FR-STA-006 | System shall handle timeout gracefully with checkpoint preservation | P0 | Timeout triggers checkpoint; job marked appropriately |\n| FR-STA-007 | System shall skip already-visited URLs on resume | P0 | No re-crawling of visited pages after resume |\n\n### **4.4 Data Extraction (spaCy)**\n\n#### **4.4.1 Entity Extraction**\n\n| ID | Requirement | Priority | Acceptance Criteria |\n| -- | ----------- | -------- | ------------------- |\n| FR-NER-001 | System shall extract company name variations | P0 | Multiple name forms captured; stored in entities table |\n| FR-NER-002 | System shall extract locations (headquarters, offices) | P0 | Location entities extracted with context |\n| FR-NER-003 | System shall extract people names with roles (CEO, founders, executives) | P0 | Person entities include role context; leadership identified |\n| FR-NER-004 | System shall extract product and service names | P0 | Product entities linked to descriptions |\n| FR-NER-005 | System shall extract organization mentions (partners, clients, investors) | P1 | Org relationships categorized |\n| FR-NER-006 | System shall extract dates (founded, milestones) | P1 | Date entities with context preserved |\n| FR-NER-007 | System shall extract monetary values (revenue, funding) | P1 | Money entities captured with currency |\n\n#### **4.4.2 Structured Data Extraction**\n\n| ID | Requirement | Priority | Acceptance Criteria |\n| -- | ----------- | -------- | ------------------- |\n| FR-STR-001 | System shall extract email patterns | P0 | Emails validated and stored |\n| FR-STR-002 | System shall extract phone numbers | P1 | Phone numbers normalized and stored |\n| FR-STR-003 | System shall extract physical addresses | P1 | Addresses parsed and stored |\n| FR-STR-004 | System shall extract social media handles | P1 | Handles linked to platforms |\n| FR-STR-005 | System shall extract tech stack indicators from job postings and about pages | P2 | Technology mentions categorized |\n\n### **4.5 AI Analysis (Claude API)**\n\n#### **4.5.1 Content Analysis**\n\n| ID | Requirement | Priority | Acceptance Criteria |\n| -- | ----------- | -------- | ------------------- |\n| FR-ANA-001 | System shall summarize company mission and value proposition | P0 | Mission summary accurate and concise |\n| FR-ANA-002 | System shall identify business model (B2B/B2C/SaaS/marketplace/etc.) | P0 | Business model correctly categorized |\n| FR-ANA-003 | System shall assess company stage (startup/growth/established/enterprise) | P0 | Stage assessment with supporting evidence |\n| FR-ANA-004 | System shall detect industry classification | P0 | Industry aligned with content |\n| FR-ANA-005 | System shall identify target market and customer segments | P1 | Target market described with evidence |\n| FR-ANA-006 | System shall extract competitive differentiators | P1 | Unique value propositions identified |\n| FR-ANA-007 | System shall identify potential red flags | P1 | Inconsistencies and concerns flagged |\n\n#### **4.5.2 Summary Generation**\n\n| ID | Requirement | Priority | Acceptance Criteria |\n| -- | ----------- | -------- | ------------------- |\n| FR-SUM-001 | System shall generate executive summary (3-4 paragraphs) | P0 | Summary readable and comprehensive |\n| FR-SUM-002 | System shall generate structured sections: Overview, Business Model, Team, Market Position, Technology, Insights, Red Flags | P0 | All sections populated with relevant content |\n| FR-SUM-003 | System shall include confidence indicators for uncertain data | P1 | Confidence scores visible for low-certainty claims |\n| FR-SUM-004 | System shall cite sources for all factual claims | P0 | Source URLs linked throughout summary |\n\n#### **4.5.3 Token Tracking**\n\n| ID | Requirement | Priority | Acceptance Criteria |\n| -- | ----------- | -------- | ------------------- |\n| FR-TOK-001 | System shall track input and output tokens per Claude API call | P0 | Token counts accurate per call |\n| FR-TOK-002 | System shall aggregate total tokens per company analysis | P0 | Total matches sum of individual calls |\n| FR-TOK-003 | System shall display cumulative token count in UI during processing | P0 | Real-time token counter updates |\n| FR-TOK-004 | System shall calculate approximate cost based on token usage | P0 | Cost estimate based on current API pricing |\n\n### **4.6 Output Generation**\n\n#### **4.6.1 Summary Document**\n\n| ID | Requirement | Priority | Acceptance Criteria |\n| -- | ----------- | -------- | ------------------- |\n| FR-OUT-001 | System shall generate 2-page markdown summary with defined structure | P0 | Summary follows template; fits 2 pages when rendered |\n| FR-OUT-002 | Summary shall include metadata: analysis date, website, industry, mode, tokens used | P0 | All metadata fields populated |\n| FR-OUT-003 | Summary shall include source page count and links to key pages | P0 | Source links functional |\n\n#### **4.6.2 Export Formats**\n\n| ID | Requirement | Priority | Acceptance Criteria |\n| -- | ----------- | -------- | ------------------- |\n| FR-EXP-001 | System shall export to Markdown (UTF-8) | P0 | Valid markdown file downloads |\n| FR-EXP-002 | System shall export to Word (.docx) with formatting | P1 | Word document opens correctly with styles |\n| FR-EXP-003 | System shall export to PDF with formatting | P1 | PDF renders correctly with styles |\n| FR-EXP-004 | System shall export to JSON with all structured data | P1 | JSON contains all analysis fields |\n\n### **4.7 Re-scan & Comparison**\n\n| ID | Requirement | Priority | Acceptance Criteria |\n| -- | ----------- | -------- | ------------------- |\n| FR-RSC-001 | System shall store content hashes for key pages | P1 | Hashes stored and retrievable |\n| FR-RSC-002 | System shall enable re-scan for any completed analysis | P1 | Re-scan button initiates new analysis |\n| FR-RSC-003 | System shall compare new scan against previous version | P1 | Changes identified and categorized |\n| FR-RSC-004 | System shall highlight changes in team, products, content | P1 | Change summary clearly shows differences |\n| FR-RSC-005 | System shall store version history (last 3 scans) | P1 | Historical versions accessible |\n\n---\n\n## **5. User Roles & Access Control**\n\n### **5.1 Role Definitions**\n\n| Role | Description | Primary Responsibilities |\n| ---- | ----------- | ------------------------ |\n| User | Primary system user conducting company research | Submit companies for analysis, monitor progress, review results, export summaries |\n| Admin | System administrator (future multi-user) | Manage configuration, view all analyses, system maintenance |\n\n*Note: V1 is designed as a single-user application. Role-based access control infrastructure is included for future multi-user expansion.*\n\n### **5.2 Feature Access Matrix**\n\n| Feature/Permission | User | Admin |\n| ------------------ | :--: | :---: |\n| Add single company | ✅ | ✅ |\n| Upload batch CSV | ✅ | ✅ |\n| View company list | ✅ | ✅ |\n| View analysis results | ✅ | ✅ |\n| Export summaries | ✅ | ✅ |\n| Pause/resume jobs | ✅ | ✅ |\n| Delete companies | ✅ | ✅ |\n| Re-scan companies | ✅ | ✅ |\n| Configure analysis settings | ✅ | ✅ |\n| View token usage | ✅ | ✅ |\n| System configuration | ❌ | ✅ |\n| View all users' data | ❌ | ✅ |\n\n### **5.3 Role-Based Workflows**\n\n**User Workflow:**\n1. Submit company or batch for analysis via web interface\n2. Monitor real-time progress on dashboard\n3. Pause/resume jobs as needed\n4. Review completed analysis summaries\n5. Export results in preferred format\n6. Re-scan companies to detect updates\n7. Delete companies no longer needed\n\n---\n\n## **6. User Interface Specifications**\n\n### **6.1 Design System**\n\n#### **Design Principles**\n\n| Principle | Description |\n| --------- | ----------- |\n| Desktop-First | Optimize for desktop productivity workflows |\n| Accessibility | WCAG 2.1 Level AA compliance |\n| Consistency | Unified component library across all views |\n| Efficiency | Minimize clicks, progressive disclosure for advanced options |\n| Clarity | Clear visual hierarchy with status-driven UI |\n\n#### **Color Palette**\n\n| Color | Hex | Usage |\n| ----- | --- | ----- |\n| Primary Blue | #2563eb | Primary actions, links |\n| Success Green | #10b981 | Completed status, success messages |\n| Warning Yellow | #f59e0b | In-progress status, warnings |\n| Error Red | #ef4444 | Failed status, errors |\n| Neutral Gray | #6b7280 | Secondary text, borders |\n| Background | #f9fafb | Page background |\n| Surface | #ffffff | Card backgrounds |\n| Text Primary | #111827 | Primary text |\n| Text Secondary | #6b7280 | Secondary text |\n\n#### **Typography**\n\n| Element | Font | Size | Weight |\n| ------- | ---- | ---- | ------ |\n| H1 | Inter / system-ui | 30px | 700 |\n| H2 | Inter / system-ui | 24px | 600 |\n| H3 | Inter / system-ui | 20px | 600 |\n| Body | System font stack | 16px | 400 |\n| Small | System font stack | 14px | 400 |\n| Code/Data | Monospace | 14px | 400 |\n\n#### **Spacing System (8px Base)**\n\n| Token | Value | Usage |\n| ----- | ----- | ----- |\n| space-1 | 4px | Tight spacing, inline elements |\n| space-2 | 8px | Default spacing |\n| space-3 | 12px | Related element spacing |\n| space-4 | 16px | Section padding |\n| space-6 | 24px | Component gaps |\n| space-8 | 32px | Large section breaks |\n\n### **6.2 Component Library**\n\n#### **Core UI Components**\n\n| Component | Purpose |\n| --------- | ------- |\n| Button | Primary, secondary, danger, and ghost variants |\n| Input | Text, URL, number inputs with validation |\n| Select | Dropdown with search and custom option |\n| Checkbox | Toggle options for external links |\n| Slider | Time limit and numeric configuration |\n| Table | Sortable, filterable data display |\n| Card | Container for company items and summaries |\n| Modal | Confirmations, raw data view |\n| Toast | Action feedback notifications |\n| Progress Bar | Visual progress indication |\n| Badge | Status indicators |\n| Tabs | Section navigation in results view |\n| Skeleton | Loading state placeholders |\n\n#### **Domain-Specific Components**\n\n| Component | Purpose | Key Props |\n| --------- | ------- | --------- |\n| CompanyCard | Display company in list | company, onView, onExport, onDelete |\n| ProgressTracker | Real-time progress display | progress: ProgressUpdate |\n| TokenCounter | Live token usage display | tokensUsed, estimatedCost |\n| AnalysisSummary | Rendered markdown summary | analysis: Analysis |\n| EntityTable | Filterable entity list | entities: Entity[], filters |\n| ExportDropdown | Format selection dropdown | onExport: (format) => void |\n| ConfigPanel | Analysis configuration form | config: AnalysisConfig, onChange |\n| BatchPreview | CSV upload preview table | rows: ParsedRow[], onConfirm |\n| VersionSelector | Historical version picker | versions: Analysis[], onSelect |\n| ChangeHighlight | Show differences between versions | comparison: ComparisonResult |\n\n### **6.3 Page Layouts**\n\n#### **Dashboard / Company List**\n\n```\n┌─────────────────────────────────────────────────────────────────┐\n│  CIRA    [+ Add Company]  [Upload CSV]              [Settings]  │\n├─────────────────────────────────────────────────────────────────┤\n│  Filters: [Status ▼] [Date Range ▼]           Search: [______]  │\n├─────────────────────────────────────────────────────────────────┤\n│  ┌───────────────────────────────────────────────────────────┐  │\n│  │ Company Name    │ Website     │ Status  │ Tokens │ Actions│  │\n│  ├───────────────────────────────────────────────────────────┤  │\n│  │ Acme Corp       │ acme.com    │ ████ 65%│ 45,230 │ ⋮      │  │\n│  │ Example Inc     │ example.com │ ✓ Done  │ 82,100 │ ⋮      │  │\n│  │ Beta Labs       │ beta.io     │ ○ Pending│   -   │ ⋮      │  │\n│  │ ...             │ ...         │ ...     │ ...    │ ...    │  │\n│  └───────────────────────────────────────────────────────────┘  │\n│                         [1] [2] [3] ... [10]                    │\n└─────────────────────────────────────────────────────────────────┘\n```\n\n#### **Progress View**\n\n```\n┌─────────────────────────────────────────────────────────────────┐\n│  ← Back    Acme Corp - acme.com                    [Pause] [✕]  │\n├─────────────────────────────────────────────────────────────────┤\n│                                                                 │\n│    Status: Analyzing...                                         │\n│    ████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░  45%          │\n│                                                                 │\n├─────────────────────────────────────────────────────────────────┤\n│  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐              │\n│  │ Pages       │  │ Entities    │  │ Tokens      │              │\n│  │   42 / 65   │  │    156      │  │   45,230    │              │\n│  └─────────────┘  └─────────────┘  └─────────────┘              │\n│                                                                 │\n│  ┌─────────────────────────────────────────────────────────────┐│\n│  │ Time Elapsed: 12:34  │  Est. Remaining: ~8 min              ││\n│  └─────────────────────────────────────────────────────────────┘│\n│                                                                 │\n│  Activity Log:                                                  │\n│  ┌─────────────────────────────────────────────────────────────┐│\n│  │ 12:45:32  Analyzing business model section...               ││\n│  │ 12:45:28  Extracted 12 entities from /team                  ││\n│  │ 12:45:20  Crawled /about-us                                 ││\n│  │ 12:45:15  Following external: linkedin.com/company/acme     ││\n│  └─────────────────────────────────────────────────────────────┘│\n└─────────────────────────────────────────────────────────────────┘\n```\n\n#### **Results View**\n\n```\n┌─────────────────────────────────────────────────────────────────┐\n│  ← Back    Acme Corp                    [Export ▼] [Re-scan]    │\n├─────────────────────────────────────────────────────────────────┤\n│  [Summary] [Entities] [Pages] [Token Usage]                     │\n├─────────────────────────────────────────────────────────────────┤\n│  ┌──────────────────────────────────┬──────────────────────────┐│\n│  │                                  │ Analysis Info            ││\n│  │  # Acme Corp - Intelligence     │ ────────────────────────  ││\n│  │  Brief                          │ Date: Jan 15, 2026       ││\n│  │                                  │ Mode: Thorough           ││\n│  │  **Analysis Date:** Jan 15...   │ Tokens: 82,100           ││\n│  │                                  │ Est. Cost: $0.82         ││\n│  │  ## Executive Summary           │                          ││\n│  │                                  │ Version: 2               ││\n│  │  Acme Corp is a B2B SaaS...    │ [View v1]                ││\n│  │                                  │                          ││\n│  │  ## Company Overview            │ Sources                  ││\n│  │  - **Founded:** 2018            │ ────────────────────────  ││\n│  │  - **HQ:** San Francisco        │ • acme.com/about         ││\n│  │  ...                            │ • acme.com/team          ││\n│  │                                  │ • linkedin.com/acme      ││\n│  └──────────────────────────────────┴──────────────────────────┘│\n└─────────────────────────────────────────────────────────────────┘\n```\n\n### **6.4 Application Structure**\n\n| View | Primary Purpose | Key Features | Component Count |\n| ---- | --------------- | ------------ | --------------- |\n| Dashboard | Company management hub | List, filter, batch actions, quick stats | ~15 |\n| Add Company | Single company input | Form validation, config options | ~10 |\n| Batch Upload | Multi-company import | CSV parsing, preview, validation | ~8 |\n| Progress | Real-time monitoring | Live updates, pause/cancel, activity log | ~12 |\n| Results | Analysis review | Markdown rendering, tabs, export, compare | ~18 |\n| Settings | Configuration | API keys, defaults, data management | ~8 |\n\n---\n\n## **7. API Specifications**\n\n### **7.1 API Design Principles**\n\n- RESTful design with consistent resource naming\n- JSON request/response bodies\n- No authentication required for V1 (single-user)\n- Rate limiting on external API calls (Claude)\n- Versioned endpoints: `/api/v1/`\n\n### **7.2 Company Endpoints**\n\n#### **POST /api/v1/companies**\n\nCreate a single company analysis job.\n\n**Request:**\n```json\n{\n  \"companyName\": \"Acme Corp\",\n  \"websiteUrl\": \"https://www.acmecorp.com\",\n  \"industry\": \"Technology\",\n  \"config\": {\n    \"analysisMode\": \"thorough\",\n    \"timeLimitMinutes\": 30,\n    \"maxPages\": 100,\n    \"maxDepth\": 3,\n    \"followLinkedIn\": true,\n    \"followTwitter\": true,\n    \"followFacebook\": false,\n    \"exclusionPatterns\": [\"/blog/*\", \"/news/*\"]\n  }\n}\n```\n\n**Response (201 Created):**\n```json\n{\n  \"success\": true,\n  \"data\": {\n    \"companyId\": \"cmp_abc123\",\n    \"status\": \"pending\",\n    \"createdAt\": \"2026-01-17T10:30:00Z\"\n  }\n}\n```\n\n#### **POST /api/v1/companies/batch**\n\nUpload CSV for batch processing.\n\n**Request:** `multipart/form-data` with CSV file\n\n**Response (201 Created):**\n```json\n{\n  \"success\": true,\n  \"data\": {\n    \"totalCount\": 10,\n    \"successful\": 9,\n    \"failed\": 1,\n    \"companies\": [\n      { \"companyName\": \"Acme Corp\", \"companyId\": \"cmp_abc123\", \"error\": null },\n      { \"companyName\": \"Bad URL Inc\", \"companyId\": null, \"error\": \"Invalid URL format\" }\n    ]\n  }\n}\n```\n\n#### **GET /api/v1/companies**\n\nList all companies with filtering and pagination.\n\n**Query Parameters:**\n- `status`: Filter by status (pending, in_progress, completed, failed, paused)\n- `sort`: Sort field (created_at, company_name, tokens_used)\n- `order`: Sort order (asc, desc)\n- `page`: Page number (default: 1)\n- `pageSize`: Items per page (default: 20, max: 100)\n\n**Response (200 OK):**\n```json\n{\n  \"success\": true,\n  \"data\": [\n    {\n      \"id\": \"cmp_abc123\",\n      \"companyName\": \"Acme Corp\",\n      \"websiteUrl\": \"https://acmecorp.com\",\n      \"status\": \"completed\",\n      \"totalTokensUsed\": 82100,\n      \"estimatedCost\": 0.82,\n      \"createdAt\": \"2026-01-15T10:30:00Z\",\n      \"completedAt\": \"2026-01-15T10:52:00Z\"\n    }\n  ],\n  \"meta\": {\n    \"total\": 45,\n    \"page\": 1,\n    \"pageSize\": 20,\n    \"totalPages\": 3\n  }\n}\n```\n\n#### **GET /api/v1/companies/:id**\n\nGet company details with analysis.\n\n**Response (200 OK):**\n```json\n{\n  \"success\": true,\n  \"data\": {\n    \"company\": {\n      \"id\": \"cmp_abc123\",\n      \"companyName\": \"Acme Corp\",\n      \"websiteUrl\": \"https://acmecorp.com\",\n      \"industry\": \"Technology\",\n      \"analysisMode\": \"thorough\",\n      \"status\": \"completed\",\n      \"totalTokensUsed\": 82100,\n      \"estimatedCost\": 0.82\n    },\n    \"analysis\": {\n      \"id\": \"ana_xyz789\",\n      \"versionNumber\": 2,\n      \"executiveSummary\": \"Acme Corp is a B2B SaaS company...\",\n      \"fullAnalysis\": { ... },\n      \"createdAt\": \"2026-01-15T10:52:00Z\"\n    },\n    \"entityCount\": 156,\n    \"pageCount\": 65\n  }\n}\n```\n\n#### **GET /api/v1/companies/:id/progress**\n\nGet real-time progress for in-progress job.\n\n**Response (200 OK):**\n```json\n{\n  \"success\": true,\n  \"data\": {\n    \"companyId\": \"cmp_abc123\",\n    \"status\": \"in_progress\",\n    \"phase\": \"analyzing\",\n    \"pagesCrawled\": 42,\n    \"pagesTotal\": 65,\n    \"entitiesExtracted\": 128,\n    \"tokensUsed\": 45230,\n    \"timeElapsed\": 754,\n    \"estimatedTimeRemaining\": 480,\n    \"currentActivity\": \"Analyzing business model section...\"\n  }\n}\n```\n\n#### **POST /api/v1/companies/:id/pause**\n\nPause an in-progress analysis.\n\n**Response (200 OK):**\n```json\n{\n  \"success\": true,\n  \"data\": {\n    \"status\": \"paused\",\n    \"checkpointSaved\": true,\n    \"pausedAt\": \"2026-01-17T11:15:00Z\"\n  }\n}\n```\n\n#### **POST /api/v1/companies/:id/resume**\n\nResume a paused analysis.\n\n**Response (200 OK):**\n```json\n{\n  \"success\": true,\n  \"data\": {\n    \"status\": \"in_progress\",\n    \"resumedFrom\": {\n      \"pagesCrawled\": 42,\n      \"entitiesExtracted\": 128,\n      \"phase\": \"analyzing\"\n    }\n  }\n}\n```\n\n#### **POST /api/v1/companies/:id/rescan**\n\nInitiate re-scan for updates.\n\n**Response (202 Accepted):**\n```json\n{\n  \"success\": true,\n  \"data\": {\n    \"newAnalysisId\": \"ana_new456\",\n    \"versionNumber\": 3,\n    \"status\": \"pending\"\n  }\n}\n```\n\n#### **GET /api/v1/companies/:id/export**\n\nExport analysis in specified format.\n\n**Query Parameters:**\n- `format`: Export format (markdown, word, pdf, json)\n- `includeRawData`: Include all extracted data (default: false)\n\n**Response:** File download with appropriate Content-Type\n\n#### **GET /api/v1/companies/:id/tokens**\n\nGet detailed token usage breakdown.\n\n**Response (200 OK):**\n```json\n{\n  \"success\": true,\n  \"data\": {\n    \"totalTokens\": 82100,\n    \"totalInputTokens\": 68500,\n    \"totalOutputTokens\": 13600,\n    \"estimatedCost\": 0.82,\n    \"byApiCall\": [\n      {\n        \"callType\": \"analysis\",\n        \"section\": \"executive_summary\",\n        \"inputTokens\": 12500,\n        \"outputTokens\": 1200,\n        \"timestamp\": \"2026-01-15T10:48:00Z\"\n      }\n    ]\n  }\n}\n```\n\n#### **GET /api/v1/companies/:id/entities**\n\nGet all extracted entities for a company.\n\n**Query Parameters:**\n- `type`: Filter by entity type\n- `minConfidence`: Minimum confidence score (0-1)\n- `page`, `pageSize`: Pagination\n\n**Response (200 OK):**\n```json\n{\n  \"success\": true,\n  \"data\": [\n    {\n      \"id\": \"ent_001\",\n      \"entityType\": \"person\",\n      \"entityValue\": \"John Smith\",\n      \"contextSnippet\": \"John Smith, CEO and founder...\",\n      \"sourceUrl\": \"https://acmecorp.com/team\",\n      \"confidenceScore\": 0.95\n    }\n  ],\n  \"meta\": { ... }\n}\n```\n\n#### **GET /api/v1/companies/:id/pages**\n\nGet all crawled pages for a company.\n\n**Response (200 OK):**\n```json\n{\n  \"success\": true,\n  \"data\": [\n    {\n      \"id\": \"pag_001\",\n      \"url\": \"https://acmecorp.com/about\",\n      \"pageType\": \"about\",\n      \"crawledAt\": \"2026-01-15T10:32:00Z\",\n      \"isExternal\": false\n    }\n  ]\n}\n```\n\n#### **GET /api/v1/companies/:id/versions**\n\nGet analysis version history.\n\n**Response (200 OK):**\n```json\n{\n  \"success\": true,\n  \"data\": [\n    {\n      \"analysisId\": \"ana_xyz789\",\n      \"versionNumber\": 2,\n      \"createdAt\": \"2026-01-15T10:52:00Z\",\n      \"tokenUsed\": 82100\n    },\n    {\n      \"analysisId\": \"ana_abc456\",\n      \"versionNumber\": 1,\n      \"createdAt\": \"2026-01-10T14:30:00Z\",\n      \"tokenUsed\": 75400\n    }\n  ]\n}\n```\n\n#### **GET /api/v1/companies/:id/compare**\n\nCompare two analysis versions.\n\n**Query Parameters:**\n- `version1`: First version number\n- `version2`: Second version number\n\n**Response (200 OK):**\n```json\n{\n  \"success\": true,\n  \"data\": {\n    \"companyId\": \"cmp_abc123\",\n    \"previousVersion\": 1,\n    \"currentVersion\": 2,\n    \"changes\": {\n      \"team\": [\n        { \"field\": \"CTO\", \"previousValue\": \"Jane Doe\", \"currentValue\": \"Bob Wilson\", \"changeType\": \"modified\" }\n      ],\n      \"products\": [],\n      \"content\": [\n        { \"field\": \"Mission statement\", \"previousValue\": null, \"currentValue\": \"...\", \"changeType\": \"added\" }\n      ]\n    },\n    \"significantChanges\": true\n  }\n}\n```\n\n#### **DELETE /api/v1/companies/:id**\n\nDelete company and all associated data.\n\n**Response (200 OK):**\n```json\n{\n  \"success\": true,\n  \"data\": {\n    \"deleted\": true,\n    \"deletedRecords\": {\n      \"pages\": 65,\n      \"entities\": 156,\n      \"analyses\": 2\n    }\n  }\n}\n```\n\n### **7.3 Configuration Endpoints**\n\n#### **GET /api/v1/config**\n\nGet current configuration.\n\n**Response (200 OK):**\n```json\n{\n  \"success\": true,\n  \"data\": {\n    \"defaults\": {\n      \"analysisMode\": \"thorough\",\n      \"timeLimitMinutes\": 30,\n      \"maxPages\": 100,\n      \"maxDepth\": 3\n    },\n    \"quickMode\": {\n      \"maxPages\": 20,\n      \"maxDepth\": 2,\n      \"followExternal\": false\n    },\n    \"thoroughMode\": {\n      \"maxPages\": 100,\n      \"maxDepth\": 3,\n      \"followExternal\": true\n    }\n  }\n}\n```\n\n#### **PUT /api/v1/config**\n\nUpdate configuration.\n\n**Request:**\n```json\n{\n  \"defaults\": {\n    \"timeLimitMinutes\": 45\n  }\n}\n```\n\n**Response (200 OK):**\n```json\n{\n  \"success\": true,\n  \"data\": { ... }\n}\n```\n\n### **7.4 Error Responses**\n\n```json\n{\n  \"success\": false,\n  \"error\": {\n    \"code\": \"VALIDATION_ERROR\",\n    \"message\": \"Validation failed\",\n    \"details\": {\n      \"websiteUrl\": [\"Invalid URL format\"]\n    }\n  }\n}\n```\n\n| Error Code | HTTP Status | Description |\n| ---------- | ----------- | ----------- |\n| VALIDATION_ERROR | 400 | Request validation failed |\n| NOT_FOUND | 404 | Company or resource not found |\n| CONFLICT | 409 | Company already exists or operation conflict |\n| INVALID_STATE | 422 | Operation not valid for current state |\n| RATE_LIMITED | 429 | Too many requests |\n| EXTERNAL_API_ERROR | 502 | Claude API or external service error |\n| INTERNAL_ERROR | 500 | Server error |\n\n---\n\n## **8. Integration Requirements**\n\n### **8.1 External System Integrations**\n\n| System | Integration Type | Purpose | Data Flow |\n| ------ | ---------------- | ------- | --------- |\n| Anthropic Claude API | REST API | AI-powered content analysis and summarization | Outbound: page content, prompts; Inbound: analysis text, token counts |\n| Target Company Websites | Web Crawling | Source data extraction | Inbound: HTML, text content |\n| LinkedIn | Web Scraping | Company profile data extraction | Inbound: profile data (when enabled) |\n| Twitter/X | Web Scraping | Company profile data extraction | Inbound: profile data (when enabled) |\n| Facebook | Web Scraping | Business page data extraction | Inbound: page data (when enabled) |\n\n### **8.2 Claude API Integration Specifications**\n\n**Authentication:**\n- API key stored in environment variable `ANTHROPIC_API_KEY`\n- Key passed via `x-api-key` header\n\n**Rate Limiting:**\n- Implement exponential backoff on 429 responses\n- Queue management for concurrent requests\n- Maximum 3 retries per request\n\n**Error Handling:**\n- Retry on transient errors (5xx, timeouts)\n- Fall back gracefully if API unavailable\n- Log all API errors for debugging\n\n**Token Management:**\n- Track input_tokens and output_tokens from response\n- Calculate cost using current pricing\n- Store per-call and aggregate metrics\n\n### **8.3 Web Crawling Integration**\n\n**Playwright Configuration:**\n- Headless browser mode\n- 30-second timeout per page\n- Viewport: 1920x1080\n- User-Agent: \"CIRA Bot/1.0\"\n\n**Rate Limiting:**\n- 1 request per second to same domain\n- Respect Crawl-delay from robots.txt\n- Exponential backoff on errors\n\n**robots.txt Compliance:**\n- Parse and cache robots.txt\n- Honor Disallow directives\n- Check before each request\n\n---\n\n## **9. Non-Functional Requirements**\n\n### **9.1 Security Requirements**\n\n| ID | Requirement | Priority | Acceptance Criteria |\n| -- | ----------- | -------- | ------------------- |\n| NFR-SEC-001 | API keys stored in environment variables, never in code | P0 | No secrets in codebase |\n| NFR-SEC-002 | Input validation on all user inputs to prevent injection | P0 | SQLi, XSS tests pass |\n| NFR-SEC-003 | SQL injection prevention via parameterized queries (SQLAlchemy ORM) | P0 | Security scan clean |\n| NFR-SEC-004 | No storage of sensitive PII beyond business information | P0 | Data audit verification |\n| NFR-SEC-005 | Secure file export with proper Content-Disposition headers | P1 | File download security verified |\n\n### **9.2 Performance Requirements**\n\n| ID | Requirement | Priority | Acceptance Criteria |\n| -- | ----------- | -------- | ------------------- |\n| NFR-PER-001 | API response time < 200ms for non-blocking operations | P0 | Load testing verification |\n| NFR-PER-002 | UI responsiveness < 200ms for user interactions | P0 | Performance monitoring |\n| NFR-PER-003 | Crawl speed of 1-2 pages/second (respecting rate limits) | P0 | Crawl timing logs |\n| NFR-PER-004 | Database queries < 100ms for most operations | P0 | Query profiling |\n| NFR-PER-005 | Export generation < 5 seconds for standard formats | P1 | Export timing tests |\n| NFR-PER-006 | Progress updates every 2 seconds during active processing | P0 | Real-time update verification |\n\n### **9.3 Availability & Reliability**\n\n| ID | Requirement | Priority | Acceptance Criteria |\n| -- | ----------- | -------- | ------------------- |\n| NFR-AVL-001 | System available 99% of uptime (excluding maintenance) | P0 | Uptime monitoring |\n| NFR-AVL-002 | Automatic retry on transient network failures (3 attempts) | P0 | Retry logic verification |\n| NFR-AVL-003 | Graceful degradation if Claude API unavailable | P0 | Failover testing |\n| NFR-AVL-004 | Data integrity with database transactions | P0 | Transaction verification |\n| NFR-AVL-005 | Resume capability works 100% after interruptions | P0 | Resume testing |\n\n### **9.4 Scalability Requirements**\n\n| ID | Requirement | Priority | Acceptance Criteria |\n| -- | ----------- | -------- | ------------------- |\n| NFR-SCA-001 | Support 100+ companies in single batch | P0 | Batch testing |\n| NFR-SCA-002 | Database optimized for 10,000+ company records | P1 | Performance at scale |\n| NFR-SCA-003 | Configurable number of concurrent Celery workers | P1 | Worker scaling tests |\n| NFR-SCA-004 | Efficient pagination for large datasets | P1 | Pagination performance |\n\n### **9.5 Accessibility Requirements**\n\n| ID | Requirement | Priority | Acceptance Criteria |\n| -- | ----------- | -------- | ------------------- |\n| NFR-ACC-001 | WCAG 2.1 Level AA compliance | P1 | Accessibility audit |\n| NFR-ACC-002 | Full keyboard navigation support | P1 | Manual testing |\n| NFR-ACC-003 | Screen reader compatibility (NVDA, VoiceOver) | P1 | Screen reader testing |\n| NFR-ACC-004 | Color contrast ratios ≥ 4.5:1 | P1 | Automated checking |\n| NFR-ACC-005 | Focus indicators visible on all interactive elements | P1 | Visual verification |\n\n### **9.6 Browser & Device Support**\n\n**Web Application:**\n- Chrome 90+\n- Firefox 88+\n- Safari 14+\n- Edge 90+\n\n**Minimum Screen Resolution:**\n- 1280x720 (desktop-first design)\n- Responsive down to 1024px width\n\n---\n\n## **10. Glossary**\n\n| Term | Definition |\n| ---- | ---------- |\n| Analysis | The AI-generated intelligence report for a company including summary and structured sections |\n| Batch Processing | Submitting multiple companies for analysis simultaneously via CSV upload |\n| Checkpoint | A saved state of crawl/analysis progress enabling resume after interruption |\n| Crawl Depth | The number of link-follows from the starting URL (homepage = depth 0) |\n| Entity | A structured piece of information extracted via NLP (person, organization, location, etc.) |\n| NER | Named Entity Recognition - NLP technique for identifying and classifying entities in text |\n| Quick Mode | Analysis configuration with lower page limits and depth for faster results |\n| Re-scan | Running a new analysis on a previously analyzed company to detect changes |\n| spaCy | Open-source NLP library used for entity extraction |\n| Thorough Mode | Analysis configuration with higher limits for comprehensive coverage |\n| Token | Unit of text processed by Claude API; used for usage tracking and cost calculation |\n\n---\n\n## **Appendix A: User Stories**\n\n### **Research Analyst Stories**\n\n> \"As a research analyst, I want to submit a batch of companies via CSV so that I can analyze multiple potential partners simultaneously.\"\n\n> \"As a research analyst, I want to monitor the real-time progress of my analysis jobs so that I can estimate when results will be ready.\"\n\n> \"As a research analyst, I want to pause and resume analysis jobs so that I can manage long-running tasks without losing progress.\"\n\n> \"As a research analyst, I want to choose between quick and thorough analysis modes so that I can balance speed versus comprehensiveness based on my needs.\"\n\n> \"As a research analyst, I want to view a structured 2-page summary so that I can quickly understand a company's key information.\"\n\n> \"As a research analyst, I want to export summaries to Word and PDF so that I can share findings with colleagues who don't use the system.\"\n\n> \"As a research analyst, I want to re-scan companies so that I can detect changes since my last analysis.\"\n\n> \"As a research analyst, I want to see token usage and cost estimates so that I can manage my API budget.\"\n\n> \"As a research analyst, I want to view the raw extracted entities so that I can verify and explore the source data.\"\n\n> \"As a research analyst, I want to follow LinkedIn company profiles so that I can get additional professional context.\"\n\n---\n\n## **Appendix B: Wireframes & Mockups**\n\nWireframe diagrams are provided in Section 6.3 (Page Layouts) using ASCII representations. High-fidelity mockups should be created during the design phase following the design system specifications in Section 6.1.\n\n---\n\n## **Appendix C: Data Retention & Privacy**\n\n| Data Type | Retention Period | Deletion Policy |\n| --------- | ---------------- | --------------- |\n| Company Records | Indefinite (user-controlled) | Deleted on user request |\n| Crawled Page Content | Indefinite (user-controlled) | Deleted with company record |\n| Extracted Entities | Indefinite (user-controlled) | Deleted with company record |\n| Analysis Summaries | Keep last 3 versions per company | Older versions auto-deleted |\n| Token Usage Logs | 1 year | Auto-deleted after retention period |\n| Export Files | 7 days | Auto-deleted from server; user retains downloads |\n\n**Privacy Considerations:**\n- System only processes publicly available information\n- No personal data beyond business context is stored\n- Users responsible for compliance with their jurisdiction's data laws\n- No data shared with third parties except Claude API for processing\n\n---\n\n## **Priority Definitions**\n\n| Priority | Definition | Implementation |\n| -------- | ---------- | -------------- |\n| **P0** | Must-have for MVP | Required before launch |\n| **P1** | Important for completeness | Implement in first iteration |\n| **P2** | Nice-to-have | Future enhancement |\n| **P3** | Future consideration | Backlog |\n\n---\n\n*— End of Document —*",
  "finalAudience": "# Audience & Jobs to Be Done\n\n## Primary Audience\n\n### Business Development & Research Professionals\n\nThe primary audience for CIRA consists of professionals who regularly need to evaluate companies as potential clients, partners, or vendors. This includes:\n\n- **Business Development Representatives** - Researching prospects before outreach\n- **Partnership Managers** - Evaluating potential strategic partners\n- **Procurement Specialists** - Conducting vendor due diligence\n- **Investment Analysts** - Preliminary screening of target companies\n- **Consultants** - Rapid company research for client engagements\n\nThese professionals share common characteristics:\n- Time-constrained with high research volume requirements\n- Need consistent, comprehensive company intelligence\n- Value structured information over raw data\n- Require shareable, professional output formats\n- Work primarily from desktop environments\n\n### Jobs to Be Done\n\nFor Business Development & Research Professionals:\n\n1. **Rapidly Understand an Unfamiliar Company**\n   - **Outcome**: Have a comprehensive mental model of what a company does, who leads it, and its market position within 30 minutes instead of 3+ hours\n   - **Context**: When preparing for a sales call, partnership meeting, or initial vendor evaluation\n\n2. **Consistently Evaluate Multiple Companies at Scale**\n   - **Outcome**: Compare 10+ companies using the same criteria and format, enabling apples-to-apples comparison\n   - **Context**: When building a qualified prospect list, evaluating RFP responses, or conducting market research\n\n3. **Build Confidence Before Important Conversations**\n   - **Outcome**: Enter meetings with verified facts about leadership, products, and recent company developments\n   - **Context**: Before pitching to a prospect, negotiating with a vendor, or presenting partnership opportunities\n\n4. **Create Professional Research Documentation**\n   - **Outcome**: Produce shareable intelligence briefs that can be distributed to colleagues, executives, or clients\n   - **Context**: When preparing for team reviews, executive briefings, or client deliverables\n\n5. **Track Companies Over Time**\n   - **Outcome**: Understand how a company has evolved since last evaluation (new leadership, products, positioning)\n   - **Context**: When re-engaging with a dormant prospect, renewing vendor contracts, or monitoring competitors\n\n## Connected Audiences\n\n### Team Leaders & Managers\n\n**Relationship**: Supervise research professionals and consume their output\n\n**Jobs to Be Done**:\n\n1. **Maintain Research Quality Standards**\n   - **Outcome**: Ensure all team members produce consistent, high-quality company research regardless of individual skill level\n   - **Context**: When onboarding new team members or standardizing department processes\n\n2. **Optimize Team Research Efficiency**\n   - **Outcome**: Reduce time spent on manual research across the team, freeing capacity for higher-value activities\n   - **Context**: When managing team workload or justifying headcount decisions\n\n### Executive Decision Makers\n\n**Relationship**: Final decision makers who rely on research summaries\n\n**Jobs to Be Done**:\n\n1. **Make Informed Go/No-Go Decisions**\n   - **Outcome**: Receive concise, actionable company intelligence that supports strategic decisions\n   - **Context**: When approving partnerships, major vendor selections, or acquisition targets\n\n---\n\n## User Personas\n\n### Persona 1: Sarah - Senior Business Development Representative\n\n- **Role**: Senior BDR at a B2B SaaS company\n- **Experience**: 5 years in sales, handles enterprise accounts\n- **Goals**: \n  - Build a pipeline of qualified enterprise prospects\n  - Personalize outreach based on company-specific insights\n  - Prepare thoroughly for discovery calls\n- **Pain Points**: \n  - Spends 2-3 hours researching each enterprise prospect manually\n  - Information scattered across LinkedIn, company websites, news articles\n  - Inconsistent research depth depending on available time\n  - Difficult to document and share research with account executives\n- **Needs**: \n  - Batch processing to research multiple prospects efficiently\n  - Structured summaries highlighting business model and key stakeholders\n  - Exportable formats for CRM notes and call prep\n\n### Persona 2: Michael - Procurement Manager\n\n- **Role**: Senior Procurement Manager at a mid-size manufacturing company\n- **Experience**: 8 years in supply chain and vendor management\n- **Goals**: \n  - Evaluate vendor stability and capabilities\n  - Conduct due diligence for contract renewals\n  - Identify potential risks in vendor relationships\n- **Pain Points**: \n  - Manual vendor research is tedious and inconsistent\n  - Difficult to track changes in vendor organizations over time\n  - No standardized format for vendor assessments\n  - Time pressure during RFP evaluation periods\n- **Needs**: \n  - Thorough analysis including company stability indicators\n  - Re-scan capability to monitor existing vendors\n  - Red flag identification for risk assessment\n  - Professional reports for procurement committee reviews\n\n### Persona 3: Elena - Strategy Consultant\n\n- **Role**: Senior Consultant at a management consulting firm\n- **Experience**: 6 years in strategy consulting\n- **Goals**: \n  - Rapidly understand client industries and competitors\n  - Prepare market landscape analyses\n  - Support due diligence for M&A engagements\n- **Pain Points**: \n  - Project timelines require fast research turnaround\n  - Quality varies when delegating research to junior analysts\n  - Manual research doesn't scale with project demands\n  - Clients expect polished, professional deliverables\n- **Needs**: \n  - Quick mode for initial screening, thorough mode for deep dives\n  - Token/cost tracking for client billing purposes\n  - Export to formats suitable for client presentations\n  - Consistent quality regardless of which team member runs the analysis\n\n---\n\n## Notes\n\n**Research Context Drivers:**\n- Company research needs arise from specific business events: new prospect identification, RFP responses, partnership discussions, contract renewals, competitive monitoring\n- Research depth requirements vary: sometimes a quick overview suffices, other times comprehensive due diligence is essential\n- Output format matters: informal internal notes vs. formal external presentations have different requirements\n\n**Key Insight:**\nThe primary value proposition is not just automation, but **consistency and comprehensiveness**. Users currently can do manual research, but the quality varies based on time available, researcher skill, and source discovery. CIRA ensures every company gets the same thorough treatment, surfacing information that might otherwise be missed.\n\n**Adoption Considerations:**\n- Users will initially run CIRA alongside manual research to validate quality\n- Trust builds as users verify AI-generated insights against known information\n- Batch processing becomes the killer feature once trust is established\n- Re-scan capability creates ongoing engagement beyond initial analysis"
}