---
phase: 01-web-crawling
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - backend/tests/test_crawl_integration.py
  - backend/tests/fixtures/crawl_fixtures.py
  - backend/tests/fixtures/__init__.py
autonomous: true

must_haves:
  truths:
    - "Pages from sitemap.xml are discovered and crawled automatically"
    - "External social links (LinkedIn, Twitter, Facebook) appear in crawl results"
    - "Crawl stops when max_pages or max_depth limits are reached"
    - "Previously visited URLs are skipped when resuming a crawl"
  artifacts:
    - path: "backend/tests/test_crawl_integration.py"
      provides: "Integration tests for full crawl pipeline"
      min_lines: 150
    - path: "backend/tests/fixtures/crawl_fixtures.py"
      provides: "Shared fixtures for crawl testing"
      min_lines: 50
  key_links:
    - from: "CrawlWorker"
      to: "SitemapParser"
      via: "sitemap URL discovery"
      pattern: "get_urls|sitemap"
    - from: "CrawlWorker"
      to: "RobotsParser"
      via: "is_allowed check before fetch"
      pattern: "is_allowed"
    - from: "CrawlWorker"
      to: "RateLimiter"
      via: "acquire before each request"
      pattern: "acquire|rate_limit"
---

<objective>
Create integration tests for the crawl pipeline that verify all components work together correctly.

Purpose: Validate that the existing crawl implementation meets Phase 1 requirements when components are integrated, catching wiring issues that unit tests miss.

Output: Integration test file with mocked network but real component integration.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/01-web-crawling/01-RESEARCH.md
@backend/app/crawlers/crawl_worker.py
@backend/app/crawlers/sitemap_parser.py
@backend/app/crawlers/robots_parser.py
@backend/app/crawlers/external_links.py
@backend/tests/conftest.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create crawl integration test fixtures</name>
  <files>backend/tests/fixtures/__init__.py, backend/tests/fixtures/crawl_fixtures.py</files>
  <action>
Create shared fixtures for crawl integration testing:

1. Create `backend/tests/fixtures/__init__.py` (empty file to make it a package)

2. Create `backend/tests/fixtures/crawl_fixtures.py` with:
   - `mock_html_responses`: Dict mapping URLs to HTML content with realistic page structure
     - Homepage with links to about, team, products, contact
     - About page with company description and LinkedIn link
     - Team page with person names
     - Products page with product list
   - `mock_sitemap_response`: Valid sitemap.xml containing all mock URLs
   - `mock_robots_response`: robots.txt allowing all paths, with sitemap declaration
   - `mock_external_links`: Dict of social profile URLs (linkedin.com/company/test, twitter.com/test)
   - `create_mock_fetcher()`: Factory function returning SimpleFetcher mock that returns appropriate HTML based on URL
   - `create_mock_http_responses()`: Factory for httpx mock responses for sitemap/robots

3. Each mock HTML should include:
   - Proper `<title>` tags
   - Internal `<a href>` links for discovery
   - At least one external social link per relevant page
   - Realistic text content (50+ words) for content hash testing

Use pytest fixtures pattern that can be imported into test files.
  </action>
  <verify>
```bash
cd /Users/stephenhollifield/Cira && python3 -c "from backend.tests.fixtures.crawl_fixtures import mock_html_responses, create_mock_fetcher; print('Fixtures import OK')"
```
  </verify>
  <done>Fixture module exists with mock data factories that simulate a realistic company website structure.</done>
</task>

<task type="auto">
  <name>Task 2: Create crawl pipeline integration tests</name>
  <files>backend/tests/test_crawl_integration.py</files>
  <action>
Create integration tests that verify the full crawl pipeline in `backend/tests/test_crawl_integration.py`:

1. **Test class: TestCrawlPipelineIntegration**
   - `test_full_crawl_discovers_pages_from_sitemap`:
     - Mock HTTP to return sitemap.xml with 5 URLs
     - Create CrawlWorker with real SitemapParser, RobotsParser, PageClassifier
     - Assert all sitemap URLs are crawled
     - Assert page_type is correctly classified (about, team, product, etc.)

   - `test_crawl_respects_robots_disallow`:
     - Mock robots.txt with `Disallow: /admin`
     - Include /admin link in page HTML
     - Assert /admin is NOT crawled
     - Assert other pages ARE crawled

   - `test_crawl_extracts_external_social_links`:
     - Include LinkedIn, Twitter, Facebook links in mock HTML
     - Assert external_links_found count > 0
     - Assert detected links have correct platform identification

   - `test_crawl_deduplicates_by_content_hash`:
     - Two URLs return identical content
     - Assert duplicates_found count is 1
     - Assert only one page stored (not both)

   - `test_crawl_respects_max_pages_limit`:
     - Set max_pages=3, provide 10 pages in sitemap
     - Assert pages_crawled <= 3
     - Assert stopped_reason is 'max_pages'

   - `test_crawl_respects_max_depth_limit`:
     - Create page chain: home -> page1 -> page2 -> page3 -> page4
     - Set max_depth=2
     - Assert page4 is NOT crawled (beyond depth)

   - `test_crawl_prioritizes_high_value_pages`:
     - Provide about, team, contact, and 5 blog pages
     - Set max_pages=4
     - Assert about, team, contact are crawled before blog pages

2. **Test class: TestCrawlCheckpointing**
   - `test_checkpoint_contains_visited_urls`:
     - Crawl 5 pages
     - Assert checkpoint.visited_urls has 5 entries

   - `test_resume_from_checkpoint_skips_visited`:
     - Create checkpoint with 3 visited URLs
     - Resume crawl
     - Assert those 3 URLs are NOT re-fetched

3. Use unittest.mock.patch for HTTP requests but real crawler component instances.
4. Each test should be independent and clean up state.
  </action>
  <verify>
```bash
cd /Users/stephenhollifield/Cira && python3 -m pytest backend/tests/test_crawl_integration.py -v --tb=short 2>&1 | head -60
```
  </verify>
  <done>All integration tests pass, verifying CRL-01 through CRL-07 requirements are met by existing implementation.</done>
</task>

</tasks>

<verification>
Run all integration tests to verify crawl pipeline works end-to-end:

```bash
cd /Users/stephenhollifield/Cira && python3 -m pytest backend/tests/test_crawl_integration.py -v
```

Expected: All tests pass, demonstrating requirements CRL-01 through CRL-07 are satisfied.
</verification>

<success_criteria>
- [ ] Fixture module provides realistic mock website data
- [ ] Integration tests cover all 7 CRL requirements
- [ ] Tests verify component wiring (not just individual components)
- [ ] All tests pass with existing implementation
- [ ] Tests are documented with requirement traceability
</success_criteria>

<output>
After completion, create `.planning/phases/01-web-crawling/01-01-SUMMARY.md`
</output>
